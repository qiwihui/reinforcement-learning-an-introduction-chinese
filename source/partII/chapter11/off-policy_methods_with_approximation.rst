第11章 \*离策略近似方法
===============================================

自第5章以来，本书主要将处理在策略和离策略学习方法作为处理广义策略迭代学习形式中固有的利用与探索之间冲突的两种替代方法。
前面的两章已经用函数近似处理了在策略情况，在本章中我们用函数近似来处理离策略情况。
与在策略学习相比，函数近似的扩展与离策略学习相比显着不同且更难。
第6章和第7章中提出的表格式离策略方法很容易扩展到半梯度算法，但这些算法并没有像在策略训练下那样强大地收敛。
在本章中，我们将探讨收敛问题，仔细研究线性函数近似理论，引入可学性概念，然后讨论具有更强收敛性保证的新算法。
最后，我们将采用改进的方法，但理论结果不会像在策略学习那样强大，也不会像实验结果一样令人满意。
在此过程中，我们将更深入地了解强化学习中的近似值，以用于在策略学习和离策略学习。

回想一下，在离策略学习中，我们寻求学习 *目标策略* :math:`\pi` 的价值函数，给定数据由于不同的 *行为策略* :math:`b`。
在预测案例中，两个策略都是静态的并已知，我们试图学习状态价值 :math:`\hat{v} \approx v_{\pi}`
或动作值 :math:`\hat{q} \approx q_{\pi}`。
在控制案例中，学习动作价值，并且两种策略通常在学习过程中发生变化── :math:`\pi` 是关于 :math:`\hat{q}` 的贪婪策略，
而 :math:`b` 是更具探索性的东西，例如关于 :math:`\hat{q}` 的 :math:`\varepsilon` -贪婪策略。

离策略学习的挑战可以分为两部分，一部分出现在表格案例中，另一部分出现在函数近似中。
挑战的第一部分与更新的目标有关（不要与目标策略混淆），第二部分与更新的分布有关。
第5章和第7章中提出的与重要性采样相关的技术涉及第一部分；
这些可能会增加方差，但在所有成功的算法，不管是表格的还是近似的都需要。
这些技术在函数近似中的扩展将在本章的第一部分中快速讨论。

由于函数近似的离策略学习挑战的第二部分需要更多的东西，因为离策略情况下的更新分布不是根据在策略分布。
在策略分布对半梯度方法的稳定性很重要。已经探索了两种一般方法来解决这个问题。
一种是再次使用重要性抽样方法，这次将更新分布转回到在策略上的分布，以便保证半梯度方法收敛（在线性情况下）。
另一种是开发真正的梯度方法，不依赖于任何特殊的稳定性分布。
我们提出了基于这两种方法的方法。这是一个前沿的研究领域，目前尚不清楚这些方法中哪一个在实践中最有效。


11.1 半梯度方法
---------------

我们首先描述在前面的章节中为离策略案例开发的方法作为半梯度方法，如何容易地扩展到函数近似。
这些方法解决了离策略学习挑战的第一部分（更改更新目标），而不是第二部分（更改更新分布）。
因此，这些方法在某些情况下可能会发散，并且在这种意义上不是合理的，但它们仍然经常被成功使用。
请记住，对于表格情况，这些方法保证稳定且渐近无偏，这对应于函数近似的特殊情况。
因此，仍然可以将它们与特征选择方法相结合，使得组合系统可以保证稳定。无论如何，这些方法很简单，因此是一个很好的起点。

在第7章中，我们描述了各种表格式离策略算法。
为了将它们转换为半梯度形式，我们只需使用近似价值函数（:math:`\hat{v}` 或 :math:`\hat{q}`）
及其梯度将更新替换为数组（:math:`V` 或 :math:`Q`）以更新权重向量（:math:`\mathbf{w}`）。
其中许多的算法使用每步重要性采样率：

.. math::

    \rho_{t} \doteq \rho_{t : t}=\frac{\pi\left(A_{t} | S_{t}\right)}{b\left(A_{t} | S_{t}\right)}
    \tag{11.1}

例如，一步状态价值算法是半梯度离策略TD(0)，这与相应的在策略算法（第203页9.3节）类似，除了添加 :math:`\rho_t`：

.. math::

    \mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \rho_{t} \delta_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
    \tag{11.2}

其中 :math:`\rho_t` 的定义取决于问题是否是回合的和折扣的，还是使用平均奖励持续的和未折扣：

.. math::

    \delta_{t} \doteq R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \text { 或者 }
    \tag{11.3}

.. math::

    \delta_{t} \doteq R_{t+1}-\overline{R}_{t}+\hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
    \tag{11.4}

对于动作价值，一步算法是半梯度预期Sarsa：

.. math::

    \begin{array}{l}
    {\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t} \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right), \text { 以及 }} \\
    {\delta_{t} \doteq R_{t+1}+\gamma \sum_{a} \pi\left(a | S_{t+1}\right) \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right), \text { or } \quad \text { (回合的) }} \\
    {\delta_{t} \doteq R_{t+1}-\overline{R}_{t}+\sum_{a} \pi\left(a | S_{t+1}\right) \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right) . \quad \text { (持续的) }}
    \end{array}

请注意，此算法不使用重要性采样。在表格情况下，很明显这是恰当的，因为唯一的示例动作是 :math:`A_t`，
在学习它的价值时，我们不必考虑任何其他动作。
对于函数近似，它不太清楚，因为一旦它们都对相同的整体近似有贡献，我们可能想要对不同的状态-动作对进行不同的加权。
正确解决这个问题等待对强化学习中函数近似理论的更透彻理解。

在这些算法的多步泛化中，状态价值和动作价值算法都涉及重要性采样。例如，半梯度Sarsa的n步版本是

.. math::

    \mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1}+\alpha \rho_{t+1} \cdots \rho_{t+n-1}\left[G_{t : t+n}-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)
    \tag{11.6}

以及

.. math::

    \begin{aligned}
    G_{t : t+n} &\doteq R_{t+1}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} \hat{q}\left(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}\right), \text { 或者 } &(\text { 持续的 })\\
    G_{t : t+n} &\doteq R_{t+1}-\overline{R}_{t}+\cdots+R_{t+n}-\overline{R}_{t+n-1}+\hat{q}\left(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}\right) &(\text { 持续的 })
    \end{aligned}

在这里，我们在处理回合的结尾时略显非正式。
在第一个等式中，:math:`k \geq T`（其中 :math:`T` 是该回合的最后一个时步）时
:math:`\rho_{k} \mathrm{S}` 应该取为1，
而如果 :math:`t+n \geq T`，:math:`G_{t:n}` 应该取 :math:`G_{t}`。

回想一下，我们在第7章中还提出了一种不涉及重要性采样的离策略算法：n步树备份算法。这是它的半梯度版本：

.. math::

    \mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1}+\alpha\left[G_{t : t+n}-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)
    \tag{11.7}

.. math::

    G_{t : t+n} \doteq \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t-1}\right)+\sum_{k=t}^{t+n-1} \delta_{k} \prod_{i=t+1}^{k} \gamma \pi\left(A_{i} | S_{i}\right)
    \tag{11.8}

其中 :math:`\rho_t` 在本页顶部为预期的Sarsa定义。
我们还在第7章中定义了一种统一所有动作价值算法的算法：n步Q(:math:`\sigma`)。
我们保留该算法的半梯度形式，以及n步状态价值算法，作为读者的练习。

*练习11.1* 将n步离策略TD（7.9）的等式转换为半梯度形式。提供关于回合和持续情况的回报的附带定义。
*练习11.2* 将n步Q(:math:`\sigma`)（7.11和7.17）的方程转换为半梯度形式。给出涵盖情节和持续情况的定义。


11.2 离策略发散例子
--------------------

在本节中，我们将开始讨论使用函数近似的离策略学习挑战的第二部分──更新的分布与在策略上的分布不匹配。
我们描述了一些有针对性的离策略学习反例，即半梯度和其他简单算法不稳定和发散的情况。

为了建立直觉，最好先考虑一个非常简单的例子。假设，可能作为较大MDP的一部分，存在两个状态，
其估计价值是函数形式 :math:`w` 和 :math:`2w`，
其中参数矢量 :math:`\mathbf{w}` 仅由单个分量 :math:`w` 组成。
如果两个状态的特征向量都是简单数（单分量向量），在这种情况下为1和2，则在线性函数近似下发生这种情况。
在第一个状态中，只有一个动作可用，并且它在转移中确定性地导致第二个状态，奖励0：

.. image:: images/simple_MDP.png

其中两个圆圈内的表达式表示两个状态的值。

假设最初 :math:`w=10`，然后转变将从估计值10的状态变为估计值20的状态。
它将看起来像是良好的转变，并且将增加 :math:`w` 以提高第一状态的估计值。
如果 :math:`\gamma` 接近1，那么TD误差将接近10，并且，
如果 :math:`\alpha=0.1`，则在尝试减小TD误差时 :math:`w` 将增加到接近11。
然而，第二个状态的估计值也将增加到接近22。如果转移再次发生，那么它将从估计值 :math:`\approx 1` 的状态到
估计值 :math:`\approx 22` 的状态，TD误差为 :math:`\approx 11` ──比以前更大，而不是更小。
它看起来更像是第一个状态被低估了，它的价值将再次增加，这次是 :math:`\approx 12.1`。
这看起来很糟糕，事实上随着进一步的更新，:math:`w` 会发散到无穷大。

要明确地看到这一点，我们必须更仔细地查看更新顺序。两个状态之间转移的TD误差是

.. math::

    \delta_{t}=R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)=0+\gamma 2 w_{t}-w_{t}=(2 \gamma-1) w_{t}

并且离策略半梯度TD(0)更新（来自（11.2））是

.. math::

    w_{t+1}=w_{t}+\alpha \rho_{t} \delta_{t} \nabla \hat{v}\left(S_{t}, w_{t}\right)=w_{t}+\alpha \cdot 1 \cdot(2 \gamma-1) w_{t} \cdot 1=(1+\alpha(2 \gamma-1)) w_{t}

请注意，重要抽样比率 :math:`\rho_{t}` 在此转移时为1，因为第一个状态只有一个可用的操作，
因此其在目标和行为策略下的概率必须均为1。在上面的最后更新中，新参数是旧参数乘以标量常量 :math:`1+\alpha(2 \gamma-1)`。
如果此常数大于1，则系统不稳定，:math:`w` 将根据其初始值转为正或负无穷大。
每当 :math:`\gamma>0.5` 时，此常数大于1。请注意，稳定性不依赖于特定的步长，只要 :math:`\alpha>0`。
较小或较大的步长会影响 :math:`w` 变为无穷大时的速率，但不会影响它是否存在。

此示例的关键是一次转移重复发生，而 :math:`w` 不会在其他转移上更新。
这可以在离策略训练下进行，因为行为策略可能会选择针对目标策略永远不会进行的其他转移的操作。
对于这些转移，:math:`\rho_{t}` 将为零，不会进行更新。然而，在在策略训练下，:math:`\rho_{t}` 总是一。
每次从 :math:`w` 状态转移到 :math:`2w` 状态，增加 :math:`w` 时，也必须从 :math:`2w` 状态转移出来。
这种转变将减少 :math:`w`，除非它是一个价值高于（因为 :math:`\gamma=1`） :math:`2w` 的状态，
然后该状态必须接着一个更高价值的状态，否则 :math:`w` 将会再次减少。
每个状态只有通过创造更高的期望才能支持这个状态。最终费用必须承担（Eventually the piper must be paid）。
在在策略情况中，必须保留未来奖励的承诺，并对系统进行控制。
但是在离策略情况中，可以做出承诺，然后在采取行动后，目标策略永远不会，忘记和原谅。

这个简单的例子说明了离策略训练可能导致发散的大部分原因，但它并不完全令人信服，因为它不完整──它只是完整MDP的一个片段。
真的有一个完整的系统不稳定吗？发散的一个简单完整的例子是 *Baird的反例*。考虑图11.1中所示的回合七状态两动作MDP。
**虚线** 动作使系统以相同的概率进入六个上边状态中的一个，而 **实线** 动作将系统带到第七个状态。
行为策略 :math:`b` 以概率 :math:`\frac{6}{7}` 和 :math:`\frac{1}{7}` 选择虚线和实线动作，
使得其下的下一状态分布是均匀的（对于所有非终结状态相同），这也是每回合的起始分布。
目标政策 :math:`\pi` 总是采取实线的行动，因此在策略的分布（对于 :math:`\pi`）集中在第七个状态。
所有转移的奖励都是零。折扣率 :math:`\gamma=0.99`。

.. figure:: images/figure-11.1.png

    **图11.1：** Baird的反例。该马尔可夫过程的近似状态价值函数具有每个状态内的线性表达式所示的形式。
    **实线** 动作通常导致第七状态，并且 **虚线** 动作通常导致其他六个状态中的一个，每个状态具有相等的概率。奖励总是零。

考虑估计由每个状态圆中所示的表达式指示的线性参数化下的状态值。
例如，最左边状态的估计值是 :math:`2 w_{1}+w_{8}`，
其中下标对应于总权重向量 :math:`\mathbf{w} \in \mathbb{R}^{8}` 的分量；
这对应于第一状态的特征向量是 :math:`\mathbf{x}(1)=(2,0,0,0,0,0,0,1)^{\top}`。
所有转移的奖励为零，因此对于所有 :math:`s`，真值函数是 :math:`v_{\pi}(s)=0`，
如果 :math:`\mathbf{w}=\mathbf{0}`，则可以精确近似。
事实上，有许多解决方案，因为有更多的分量权重向量（8）比非终止状态（7）。
此外，该组特征向量的集合 :math:`\{\mathbf{x}(s) : s \in \mathcal{S}\}` 是线性独立。
在所有这些方面，该任务似乎是线性函数近似的有利情况。

如果我们将半梯度TD(0)应用于此问题（11.2），则权重会发散到无穷大，如图11.2（左）所示。
任何正步长都会出现不稳定性，无论多小。
事实上，如果在动态规划（DP）中完成预期的更新，甚至会发生这种情况，如图11.2（右）所示。
也就是说，如果使用DP（基于期望的）目标，以半梯度方式同时为所有状态更新权重向量 :math:`\mathbf{w}_{k}`：

.. math::

    \mathbf{w}_{k+1} \doteq \mathbf{w}_{k}+\frac{\alpha}{|\mathcal{S}|} \sum_{s}\left(\mathbb{E}_{\pi}\left[R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{k}\right) | S_{t}=s\right]-\hat{v}\left(s, \mathbf{w}_{k}\right)\right) \nabla \hat{v}\left(s, \mathbf{w}_{k}\right)
    \tag{11.9}

在这种情况下，没有随机性，也没有异步，就像在传统的DP更新中一样。
除了使用半梯度函数近似之外，该方法是常规的。但系统仍然不稳定。

.. figure:: images/figure-11.2.png

    **图11.2：** Baird反例的不稳定性证明。示出了两个半梯度算法的参数矢量 :math:`\mathbf{w}` 的分量的演变。
    步长为 :math:`\alpha=0.01`，初始重量为 :math:`\mathbf{w}=(1,1,1,1,1,1,10,1)^{\top}`。

如果我们只改变Baird反例中DP更新的分布，从均匀分布到在策略分布（通常需要异步更新），那么收敛就保证了一个误差受限于（9.14）的解决方案。
这个例子很引人注目，因为所使用的TD和DP方法可以说是最简单和最好理解的自举方法，所使用的线性，半梯度方法可以说是最简单和最容易理解的函数近似。
该示例表明，如果不根据策略分布进行更新，即使最简单的自举和函数近似组合也可能不稳定。

还有一些类似于Baird的反例，显示出Q-learning的发散。这引起关注，因为不然的话Q-learning具有所有控制方法的最佳收敛保证。
相当多的努力已经尝试找到解决这个问题的方法，或者获得一些较弱但仍然可行的保证。
例如，只要行为策略足够接近目标策略，例如，当它是 :math:`\varepsilon` -贪婪策略时，就有可能保证Q-learning的收敛。
据我们所知，在这种情况下，Q-learning从来没有发现过发散，但是没有理论分析。在本节的其余部分，我们提出了其他一些已被探索过的想法。

假设在Baird的反例中，我们实际上将价值函数一直改为最佳的最小二乘近似，而不是在每次迭代中只朝着预期的一步回报迈出一步。
这会解决不稳定问题吗？当然，如果特征向量 :math:`\{\mathbf{x}(s) : s \in \mathcal{S}\}` 形成一个线性独立的集合，
就像在Baird的反例中那样，因为那样在每次迭代时都可以进行精确逼近，并且该方法简化为标准的表格DP 。
但当然，这里的重点是考虑 *无法* 获得精确解决方案的情况。在这种情况下，即使在每次迭代时形成最佳近似值，也不能保证稳定性，如示例所示。

.. figure:: images/example-11.1.png
    :width: 250px
    :align: right

**例11.1：Tsitsiklis和Van Roy的反例** 这个例子表明，即使在每一步找到最小二乘解，线性函数近似也不适用于DP。
通过将 :math:`w \text{-to-} 2w` 示例（来自本节前面部分）扩展为具有终止状态来形成反例，如右图所示。
如前所述，第一状态的估计值是 :math:`w`，第二状态的估计值是 :math:`2w`。
所有转换的奖励为零，因此两个状态的真值均为零，:math:`w=0` 时可以正确表示。
如果我们在每一步设置 :math:`w_{k+1}`，
以便最小化估计值与预期的一步回报之间的 :math:`\overline{\mathrm{VE}}`，然后我们有

.. math::

    \begin{aligned}
    w_{k+1} &=\underset{w \in \mathbb{R}}{\operatorname{argmin}} \sum_{s \in \mathcal{S}}\left(\hat{v}(s, w)-\mathbb{E}_{\pi}\left[R_{t+1}+\gamma \hat{v}\left(S_{t+1}, w_{k}\right) | S_{t}=s\right]\right)^{2} \\
    &=\underset{w \in \mathbb{R}}{\operatorname{argmin}}\left(w-\gamma 2 w_{k}\right)^{2}+\left(2 w-(1-\varepsilon) \gamma 2 w_{k}\right)^{2} \\
    &=\frac{6-4 \varepsilon}{5} \gamma w_{k} & \text{(11.10)}
    \end{aligned}

当 :math:`\gamma>\frac{5}{6-4 \varepsilon}` 和 :math:`w_{0} \neq 0` 时
序列 :math:`\left\{w_{k}\right\}` 收敛。

另一种尝试防止不稳定的方法是使用特殊方法进行函数近似。
特别是，对于不从观察到的目标推断的函数近似方法，保证了稳定性。
这些方法称为 *平均器（averagers）*，包括最近邻方法和局部加权回归，但不是流行的方法，如铺片编码和人工神经网络（ANN）。

*练习11.3（编程）* 将一步半梯度Q-learning应用于Baird的反例，并凭经验证明其权重不同。


11.3 致命的三元组
------------------

到目前为止，我们的讨论可以总结为，只要我们将以下三个要素结合起来，构成我们称之为 *致命的三元组*，就会产生不稳定和分歧的危险：

**函数近似** 从比内存和计算资源（例如，线性函数近似或ANN）大得多的状态空间泛化的强大和可扩展的方式。

**自举** 更新目标包括现有估计（如动态编程或TD方法），而不是完全依赖实际奖励和完整回报（如MC方法）。

**离策略训练** 除了目标策略产生的转移分布之外的训练。扫描状态空间并统一更新所有状态，如动态规划，
不遵循目标策略，是离策略训练的一个例子。

特别要注意的是，危险 *不是* 由于控制或广义策略迭代造成的。
这些案例的分析比较复杂，但只要包含致命三元组的所有三个要素，就会在更简单的预测案例中产生不稳定性。
危险也 *不是* 由于学习或对环境的不确定性造成的，因为它在规划方法中同样强烈地发生，例如动态规划，其中环境是完全已知的。

如果存在致命三元组的任何两个元素，但不是全部三个元素，则可以避免不稳定。
那么，检查三者来看是否有任何可以放弃的东西是很自然的。

在这三者中，*函数近似* 最明显不能放弃。我们需要能够扩展到大问题和极具表现力的方法。
我们至少需要具有许多特征和参数的线性函数近似。
状态聚合或非参数方法的复杂性随着数据的增长而变得太弱或太昂贵。
诸如LSTD的最小二乘法具有二次复杂性，因此对于大问题而言太昂贵。

在没有 *自举* 的情况下，可以以计算和数据效率为代价。也许最重要的是计算效率的损失。
蒙特卡罗（非自举）方法需要内存来保存在进行每次预测和获得最终返回之间发生的所有结果，并且一旦获得最终返回就完成所有计算。
这些计算问题的成本在串行冯·诺依曼计算机上并不明显，但是在专用硬件上明显。
使用自举和资格迹（第12章），可以在生成数据的时间和地点处理数据，然后再也不需要再使用。
通过自举实现的通信和内存节省是非常好的。

放弃 *自举* 对数据效率的损失也很大。我们已经反复看到过这种情况，例如第7章（图7.2）和第9章（图9.2），
其中某些程度的自举比随机行走预测任务上的蒙特卡罗方法表现要好得多，
而在第10章中，相同的是在陡坡汽车控制任务中看到（图10.4）。
许多其他问题表明使用自举更快地学习（例如，见图12.14）。
自举通常会导致更快的学习，因为它允许学习利用状态属性，即在返回状态时识别状态的能力。
另一方面，自举可能会削弱对状态表示不佳的问题的学习并导致泛化效果差。
（例如，俄罗斯方块似乎就是这种情况，参见Şim̧sek，Alg orta和Kothiyal，2016）。
不佳的状态表示也可能导致偏差；这就是自举方法渐近近似质量较差的原因（公式9.14）。
总的来说，自举能力必须被认为是非常有价值的。
有时可能会选择不使用它而选择长n步更新（或大的自举参数，:math:`\lambda \approx 1`；参见第12章），但通常自举会大大提高效率。
这是我们非常希望保留在我们的工具包中的能力。

最后，是 *离策略学习*；我们可以放弃吗？在策略方法通常是充足的。
对于不基于模型的强化学习，人们可以简单地使用Sarsa而不是Q-learning。
离策略方法从目标策略中释放行为。这可以被认为是一种吸引人的便利，但不是必需的。
但是，离策略学习对于其他预期的用例至关重要，我们在本书中尚未提及但可能对创建强大智能个体的更大目标很重要。

在这些用例中，个体不仅学习单个价值函数和单个策略，而且并行学习大量这些数据。
有广泛的心理证据表明人和动物学会预测许多不同的感官事件，而不仅仅是奖励。
我们可以对不寻常的事件感到惊讶，并纠正我们对它们的预测，即使它们具有中性效价（既不好也不坏）。
这种预测可能是世界预测模型的基础，例如用于规划的模型。
我们预测在转动眼球后我们会看到什么，走回家需要多长时间，在篮球中跳投的可能性，以及我们从承担新项目中获得的满足感。
在所有这些情况下，我们想要预测的事件取决于我们以某种方式行事。要同时学习它们，需要从一种经验中学习。
有许多目标策略，因此一种行为策略不能与所有策略相等。
然而，并行学习在概念上是可能的，因为行为策略可能部分地与许多目标策略重叠。要充分利用这一点，需要进行离策略学习。


11.4 线性价值函数几何
-----------------------


11.5 Bellman误差中的梯度下降
------------------------------


11.6 Bellman误差是不可学习的
------------------------------


11.7 梯度TD方法
-----------------


11.8 强调TD方法
---------------


11.9 减小误差
---------------


11.10 总结
---------------


书目和历史评论
---------------
