第6章 时序差分学习
==================

如果必须将一个想法确定为强化学习的核心和新颖，那么毫无疑问它将是 *时序差分* （TD）学习。
TD学习是蒙特卡洛思想和动态规划（DP）思想的结合。与蒙特卡洛方法一样，TD方法可以直接从原始经验中学习，而无需环境动态模型。
与DP一样，TD方法部分基于其他学习估计更新估计，而无需等待最终结果（它们是自举）。
TD，DP和蒙特卡洛方法之间的关系是强化学习理论中反复出现的主题；本章是我们开始探索这个关系。
在我们完成之前，我们将看到这些想法和方法相互融合，并且可以以多种方式组合。
特别是，在第7章中，我们介绍了n步算法，它提供了从TD到蒙特卡洛方法的桥梁，
在第12章中我们介绍了 TD(:math:`\lambda`) 算法，它无缝地统一了它们。

像往常一样，我们首先关注策略评估或 *预测* 问题，即估算给定策略 :math:`\pi` 的价值函数 :math:`v_\pi` 的问题。
对于控制问题（找到最优策略），DP、TD和蒙特卡洛方法都使用广义策略迭代（GPI）的一些变体。
方法的差异主要在于它们对预测问题的方法的差异。


6.1 TD预测
-------------

TD和蒙特卡罗方法都使用经验来解决预测问题。对于基于策略 :math:`\pi` 的一些经验，
两种方法都更新了他们对该经验中发生的非终结状态 :math:`S_t` 的 :math:`v_\pi` 的估计 :math:`V`。
粗略地说，蒙特卡罗方法一直等到访问后的回报已知，然后使用该回报作为 :math:`V(S_t)` 的目标。
适用于非平稳环境的简单的每次访问蒙特卡罗方法是

.. math::

    V(S_{t}) \leftarrow V(S_{t})+\alpha\left[ G_{t}-V(S_{t})\right]
    \tag{6.1}

其中 :math:`G_t` 是跟随时间t的实际回报，:math:`\alpha` 是一个恒定的步长参数（参见方程2.4）。
我们将此方法称为恒定 :math:`\alpha` MC。
蒙特卡罗方法必须等到回合结束才能确定 :math:`V(S_t)` 的增量（这时只有 :math:`G_t` 已知），
TD方法需要等到下一个时间步。 在时间 :math:`t+1`，它们立即形成目标并
使用观察到的奖励 :math:`R_{t+1}`和估计 :math:`V(S_{t+1})` 进行有用的更新。
最简单的TD方法立即进行更新

.. math::

    V(S_{t}) \leftarrow V(S_{t})+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})\right]
    \tag{6.2}

过渡到 :math:`S_{t+1}` 并接收 :math:`R_{t+1}`。
在实际中，蒙特卡洛更新的目标是 :math:`G_t`，而TD更新的目标是 :math:`R_{t+1} + \gamma V(S_{t+1})`。
这种TD方法称为 *TD(0)* 或 *一步TD*，因为它是第12章和第7章中开发的 TD(:math:`\lambda`)和n步TD方法的特例。
下面的方框完全以程序形式给出了TD(0)。

.. admonition:: 表格TD(0)估计 :math:`v_\pi`
    :class: important

    输入：要评估策略 :math:`\pi`

    算法参数：步长 :math:`\alpha in (0,1]`

    对所有 :math:`s \in \mathbb{S}^{+}`，除了 :math:`V(终点)=0`，任意初始化 :math:`V(s)`

    对每个回合循环：

        初始化 :math:`S`

        对回合的每一步循环：

            :math:`A \leftarrow` 由 :math:`\pi` 给出 :math:`S` 的动作

            采取动作 :math:`A`，观察 :math:`R`，:math:`S^{\prime}`

            :math:`V(S) \leftarrow V(S)+\alpha\left[R+\gamma V(S^{\prime})-V(S)\right]`

            :math:`S \leftarrow S^{\prime}`

        直到 :math:`S` 是终点

因为TD(0)部分基于现有估计进行更新，所以我们说它是一种 *自举（bootstrapping）* 方法，就像DP一样。
我们从第3章知道

.. math::

    \begin{align}
    v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]  \tag{6.3}\\
    &=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] & (由(3.9))\\
    &=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right] \tag{6.4}
    \end{align}

粗略地说，蒙特卡罗方法使用（6.3）的估计作为目标，而DP方法使用（6.4）的估计作为目标。
蒙特卡洛目标是估计，因为（6.3）中的预期值未知；使用样本回报来代替实际预期回报。
DP目标是一个估计，不是因为完全由环境模型提供的预期值，而是因为 :math:`v_{\pi}(S_{t+1})` 未知，
且使用当前估计值 :math:`V(S_{t+1})` 来代替。
TD目标是估计原因有两个：它在（6.4）中对预期值进行采样，*并* 使用当前估计值 :math:`V` 而不是真实 :math:`v_\pi`。
因此，TD方法将蒙特卡罗的采样与DP的自举相结合。
正如我们将要看到的那样，通过谨慎和想象，这将使我们在获得蒙特卡罗和DP方法的优势方面走得很远。

.. figure:: images/TD(0).png
    :align: right
    :width: 50px

右侧是表格TD(0)的备份图。
备份图顶部的状态节点的值估计基于从它到紧接的状态的一个样本转换而更新。
我们将TD和蒙特卡洛更新称为样本更新，因为它们涉及展望样本后继状态（或状态-动作对），
使用后继值和相应的奖励来计算备份值（?），然后相应地更新原始状态（或状态-动作对）的值。
*样本* 更新与DP方法的 *预期* 更新不同，因为它们基于单个样本后继，而不是基于所有可能后继的完整分布。

最后，请注意在TD(0)更新中，括号中的数量是一种误差，
衡量 :math:`S_t` 的估计值与更好的估计 :math:`R_{t+1} + \gamma V(S_{t+1})` 之间的差异。
这个数量称为 *TD误差*，在整个强化学习过程中以各种形式出现：

.. math::

    \delta_{t} \doteq R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)
    \tag{6.5}

请注意，每次TD误差都是 *当时估算* 的误差。因为TD误差取决于下一个状态和下一个奖励，所以直到一个步骤之后才可用。
也就是说，:math:`\delta_{t}` 是 :math:`V(S_{t+1})` 中的误差，在时间 :math:`t+1` 可用。
还要注意，如果列表 :math:`V` 在回合期间没有改变（因为它不是蒙特卡罗方法(?)），那么蒙特卡罗误差可以写成TD误差的和：

.. math::

    \begin{align}
    G_{t}-V\left(S_{t}\right) &=R_{t+1}+\gamma G_{t+1}-V\left(S_{t}\right)+\gamma V\left(S_{t+1}\right)-\gamma V\left(S_{t+1}\right) & (由(3.9)) \\
    &=\delta_{t}+\gamma\left(G_{t+1}-V\left(S_{t+1}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2}\left(G_{t+2}-V\left(S_{t+2}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}\left(G_{T}-V\left(S_{T}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}(0-0) \\
    &=\sum_{k=t}^{T-1} \gamma^{k-t} \delta_{k} \tag{6.6}
    \end{align}

如果在回合期间更新 :math:`V` （因为它在TD(0)中），则此恒等式不准确，但如果步长很小，那么它可能仍然保持近似。
这种恒等式的一般化在时序差分学习的理论和算法中起着重要作用。

*练习6.1* 如果 :math:`V` 在回合中发生变化，那么（6.6）只能保持近似；等式两边的区别是什么？
设 :math:`V_t` 表示在TD误差（6.5）和TD更新（6.2）中在时间 :math:`t` 使用的状态值列表。
重做上面的推导以确定必须添加到TD误差总和的额外量，以便等于蒙特卡罗误差。

**例 6.1 开车回家** 每天下班回家后，你都会尝试预测回家需要多长时间。
当你离开办公室时，你会记下时间，星期几，天气以及其他可能相关的内容。
这个星期五你正好在6点钟离开，你估计要回家需要30分钟。当你到达你的车是6:05，你注意到开始下雨了。
在雨中交通通常较慢，所以你需要花费35分钟，或者总共40分钟。十五分钟后，你及时完成了旅程的高速公路部分。
当你驶出高速进入第二部分道路时，你将总旅行时间的估计值减少到35分钟。
不幸的是，此时你被困在一辆慢卡车后面，而且道路太窄而无法通过。
你最终不得不跟随卡车，直到6:40你转到住的小街。三分钟后你就回家了。因此，状态，时间和预测的顺序如下：

======================= ==================== ================= ===============
状态                      经过时间（分钟）        预测到的时间       预计总时间
======================= ==================== ================= ===============
周五6点离开办公室            0                      30              30
到达车，下雨                 5                      35              40
驶出高速公路                 20                     15              35
第二条路，在卡车后面          30                     10              40
进入家的街道                 40                     3              43
到家                        43                     0              43
======================= ==================== ================= ===============

这个例子中的奖励是旅程每一段的经过时间 [1]_。我们不打折（:math:`\gamma=1`），因此每个状态的回报是从该状态开始的实际时间。
每个状态的价值是 *预期的* 时间。第二列数字给出了遇到的每个状态的当前估计值。

查看蒙特卡罗方法操作的一种简单方法是绘制序列上预测的总时间（最后一列），如图6.1（左）所示。
红色箭头表示常量 :math:`\alpha` MC方法（6.1）推荐的预测变化，其中 :math:`\alpha=1`。
这些正是每个状态的估计值（预测的行走时间）与实际返回（实际时间）之间的误差。
例如，当你离开高速公路时，你认为回家仅需15分钟，但实际上需要23分钟。
公式6.1适用于此点，并确定驶出公路后的估计时间的增量。
误差 :math:`G_t - V(S_t)` 此时为8分钟。假设步长参数 :math:`\alpha` 为 :math:`1/2`。
然后，由于这种经验，退出高速公路后的预计时间将向上修改四分钟。
在这种情况下，这可能是一个太大的变化；卡车可能只是一个不幸的中断。
无论如何，只有在你到家之后才能进行变更。只有在这一点上你才知道任何实际的回报。

.. figure:: images/figure-6.1.png

    **图6.1** 通过蒙特卡罗方法（左）和TD方法（右）在开车回家示例中推荐的变化。

在学习开始之前，是否有必要等到最终结果已知？
假设在另一天你再次估计离开你的办公室时需要30分钟才能开车回家，但是你会陷入大规模的交通堵塞之中。
离开办公室后二十五分钟，你仍然在高速公路上等待。你现在估计还需要25分钟才能回家，共计50分钟。
当你在车流中等待时，你已经知道你最初估计的30分钟过于乐观了。
你必须等到回家才增加对初始状态的估计吗？根据蒙特卡罗的方法，你必须，因为你还不知道真正的回报。

另一方面，根据TD方法，你可以立即学习，将初始估计值从30分钟转移到50分。
事实上，每个估计值都会转移到紧随其后的估计值。
回到驾驶的第一天，图6.1（右）显示了TD规则（6.2）推荐的预测变化
（如果 :math:`\alpha=1`，这些是规则所做的更改）。
每个误差与预测随时间的变化成比例，即与预测的 *时序差分* 成比例。

除了在车流中等待你做点什么之外，还有几个计算原因可以解释为什么根据你当前的预测学习是有利的，
而不是等到你知道实际回报时才终止。我们将在下一节简要讨论其中的一些内容。

*练习6.2* 这是一个练习，以帮助你发展直觉，了解为什么TD方法通常比蒙特卡罗方法更有效。
考虑开车回家示例以及如何通过TD和蒙特卡罗方法解决它。你能想象一个TD更新平均比蒙特卡罗更新更好的情景吗？
举一个示例场景 - 过去经验和当前状态的描述 - 你期望TD更新更好。这里有一个提示：假设你有很多下班开车回家的经验。
然后你搬到一个新的建筑物和一个新的停车场（但你仍然在同一个地方进入高速公路）。现在你开始学习新建筑的预测。
在这种情况下，你能看出为什么TD更新可能会好得多，至少初始是这样吗？在原始场景中发生同样的事情可能吗？


6.2 TD预测方法的优势
---------------------

TD方法部分基于其他估计更新其估计。他们通过猜测来学习猜测 - 他们 *引导*。这是一件好事吗？
TD方法与蒙特卡罗和DP方法相比有哪些优势？开发和回答这些问题将涉及本书的其余部分以及更多内容。
在本节中，我们简要地预测一些答案。

显然，TD方法比DP方法具有优势，因为它们不需要环境模型，其奖励和下一状态概率分布。

TD方法相对于蒙特卡罗方法的下一个最明显的优势是它们自然地以在线，完全递增的方式实现。
使用蒙特卡罗方法，必须等到回合的结束，因为只有这样才能知道回报，而使用TD方法，只需要等待一个时间步。
令人惊讶的是，这通常是一个重要的考虑因素。一些应用程序有很长的回合，所以延迟所有学习直到回合结束太慢。
其他应用程序是持续的任务，根本没有回合。最后，正如我们在前一章中所提到的，
一些蒙特卡罗方法必须忽略或折扣采取实验行动的事件，这可能会大大减慢学习速度。
TD方法不太容易受到这些问题的影响，因为无论采取何种后续行动，它们都会从每次转变中学习。

但TD方法听起来有效吗？当然，从下一个中学习一个猜测是方便的，而不是等待实际的结果，
但我们仍然可以保证收敛到正确的答案吗？令人高兴的是，答案是肯定的。
对于任何固定策略 :math:`\pi`，已经证明TD(0)收敛到 :math:`v_{\pi}`，
如果它足够小，则表示恒定步长参数，如果步长参数按照通常随机近似条件（2.7）减小，则概率为1（译者注：这句没太明白）。
大多数收敛证明仅适用于上面（6.2）所述算法的基于表格的情况，但是一些也适用于一般线性函数逼近的情况。
这些结果将在9.4节的更一般性设置中讨论。

如果TD和蒙特卡罗方法渐近地收敛到正确的预测，那么自然下一个问题是“哪个首先收敛到那里？”
换句话说，哪种方法学得更快？哪种方法使得有限数据的使用更加有效？目前，这是一个悬而未决的问题，
即没有人能够在数学上证明一种方法比另一种方法收敛得更快。事实上，甚至不清楚说出这个问题的最恰当的正式方式是什么！
然而，在实践中，通常发现TD方法比常数- :math:`\alpha` MC方法在随机任务上收敛得更快，如例6.2所示。

.. admonition:: 例6.2 随机行走
    :class: important

    在这个例子中，我们在应用于以下马尔可夫奖励过程时，凭经验比较TD(0)和常数- :math:`alpha` MC的预测能力：

    .. figure:: images/Random-Walk-Markov-reward-process.png

    *马尔可夫奖励过程* （MRP）是没有行动的马尔可夫决策过程。我们经常在关注预测问题时使用MRP，
    其中不需要将由环境引起的动态与由个体引起的动态区分开来。
    在该MRP中，所有回合以中心状态 :math:`C` 开始，然后以相同的概率在每一步上向左或向右前进一个状态。
    回合终止于最左侧或最右侧。当回合在右边终止时，会产生 :math:`+1` 的奖励；所有其他奖励都是零。
    例如，典型的回合可能包含以下状态和奖励序列：:math:`C, 0, B, 0, C, 0, D, 0, E, 1`。
    因为此任务是未折扣的，所以每个状态的真实价值是从该状态开始在右侧终止的概率。
    因此，中心状态的真值是 :math:`v_\pi(C)=0.5`。所有状态 :math:`A` 到 :math:`E` 的
    真实价值都是 :math:`\frac{1}{6}`，:math:`\frac{2}{6}`，:math:`\frac{3}{6}`，
    :math:`\frac{4}{6}` 和 :math:`\frac{5}{6}`。

    .. figure:: images/random-walk-comparison.png

    上面的左图显示了在TD(0)的单次运行中在不同数量的回合之后学习的价值。
    100回合之后的估计值与它们的真实值接近 - 具有恒定的步长参数（在此示例中 :math:`\alpha=0.1`），
    这些值随着最近一个回合的结果而无限地波动。右图显示了两种方法对于各种 :math:`\alpha` 值的学习曲线。
    显示的性能度量是学习的值函数和真值函数之间的均方根（RMS）误差，在五个状态上取平均值，
    然后在超过100次运行上平均。在所有情况下，对于所有 :math:`s`，近似值函数被初始化为中间值 :math:`V(s)=0.5`。
    在这项任务中，TD方法始终优于MC方法。

*练习6.3* 从随机游走示例的左图中显示的结果看来，第一回合仅导致 :math:`V(A)` 的变化。
这告诉你第一回合发生了什么？为什么只有这一状态的估计值发生了变化？确切地说它改变了多少？

*练习6.4* 随机游走示例右图中显示的特定结果取决于步长参数 :math:`\alpha` 的值。
如果使用更广范围的 :math:`\alpha` 值，您认为关于哪种算法更好的结论是否会受到影响？
是否存在不同的固定值 :math:`\alpha`，其中任何一种算法的表现都要比显示的好得多？为什么或者为什么不？

*\*练习6.5* 在随机游走示例的右图中，TD方法的RMS误差似乎下降然后再上升，特别是在 :math:`\alpha` 高时。
可能是什么导致了这个？你认为这总是会发生，或者它可能是近似值函数初始化的函数吗？

*练习6.6* 在例6.2中，我们说状态 :math:`A` 到 :math:`E` 随机游走示例的
真实值是 :math:`\frac{1}{6}`，:math:`\frac{2}{6}`，:math:`\frac{3}{6}`，
:math:`\frac{4}{6}` 和 :math:`\frac{5}{6}`。描述至少两种不同的方式相说明这些可以计算出来。
您认为我们实际使用哪个？为什么？


6.3 TD(0)的最优
------------------


6.4 Sarsa：在策略TD控制
------------------------


6.5 Q-学习：离策略TD控制
-----------------------------


6.6 预期的Sarsa
---------------


6.8 最大化偏差和双重学习
-------------------------


6.9 游戏，后遗症和其他特殊情况
------------------------------


6.10 总结
-----------


书目和历史评论
---------------


.. [1]
    如果这是一个控制问题，目的是最大限度地缩短旅行时间，那么我们当然会将奖励作为经过时间的 *负* 影响。
    但是因为我们只关注预测（策略评估），所以我们可以通过使用正数来保持简单。
