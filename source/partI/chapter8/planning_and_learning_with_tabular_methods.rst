第8章 表格方法规划和学习
========================

在本章中，我们开发了强化学习方法的统一视图，这些方法需要环境模型，例如动态规划和启发式搜索，
以及可以在没有模型的情况下使用的方法，例如蒙特卡罗和时序差分方法。
这些分别称为 *基于模型（model-based）* 和 *不基于模型（model-free）*的强化学习方法。
基于模型的方法依赖于 *规划（planning）* 作为其主要组成部分，而不基于模型方法主要依赖于 *学习（learning）*。
虽然这两种方法之间存在着真正的差异，但也存在很大的相似之处。
特别是，这两种方法的核心是价值函数的计算。此外，所有方法都基于展望未来事件，计算备份值，然后将其用作近似值函数的更新目标。
在本书的前面，我们将蒙特卡洛和时序差分方法作为不同的替代方法，然后展示了如何通过n步方法统一它们。
我们在本章中的目标是基于模型和不基于模型方法的类似集成。
在前面的章节中已经确定了这些不同之处，我们现在探讨它们可以混合的程度。


8.1 模型和规划
-----------------

通过环境 *模型*，我们指的是个体可以用来预测环境如何响应其行为的任何事物。
给定状态和动作，模型产生对结果下一状态和下一奖励的预测。
如果模型是随机的，那么有几种可能的下一个状态和下一个奖励，每个都有一定的发生概率。
一些模型描述了所有可能性及其概率；这些我们称之为 *分布模型*。
其他模型只产生一种可能性，根据概率进行采样；这些我们称之为 *采样模型*。
例如，考虑对十几个骰子的总和进行建模。
分布模型将产生所有可能的总和及其发生的概率，而样本模型将产生根据该概率分布绘制的单个总和。
在动态规划中假设的模型──MDP动力学的估计 :math:`p\left(s^{\prime}, r | s, a\right)` ──是分布模型。
第5章中二十一点示例中使用的模型是一个采样模型。分布模型比样本模型更强大，因为它们可以始终用于产生样本。
但是，在许多应用中，获取样本模型比分布模型容易得多。十几个骰子就是一个简单的例子。
编写一个计算机程序来模拟骰子滚动并返回总和是很容易的，但是找出所有可能的总和及其概率更难且更容易出错。

模型可用于模仿或模拟体验。给定起始状态和动作，样本模型产生可能的转换，分布模型生成由其发生概率加权的所有可能转换。
给定起始状态和策略，样本模型可以产生整个回合，分布模型可以生成所有可能的回合及其概率。
在这两种情况下，我们都说模型用于 *模拟* 环境并产生 *模拟经验*。

在不同领域中，*规划* 这个词以几种不同的方式使用。
我们使用该术语来指代将模型作为输入并生成或改进与建模环境交互的策略的任何计算过程：

.. math::

    \text{模型} \stackrel{规划}{\longrightarrow} \text{策略}

在人工智能中，根据我们的定义，有两种不同的规划方法。*状态空间规划*，包括我们在本书中采用的方法，
主要被视为通过状态空间搜索最优策略或目标的最佳路径。动作导致从状态到状态的转换，并且价值函数在状态上计算。
在我们所说的 *规划空间规划* 中，规划是通过规划空间进行搜索。
操作将一个规划转换为另一个规划，并在规划空间中定义价值函数（如果有）。
规划空间规划包括演化方法和“偏序规划”，这是人工智能中的一种常见规划，其中步骤的排序在规划的所有阶段都没有完全确定。
规划空间方法难以有效地应用于强化学习中关注的随机序列决策问题，我们不会进一步考虑它们（但参见，例如，Russell和Norvig，2010）。

我们在本章中提出的统一观点是，所有的状态空间规划方法都有一个共同的结构，这种结构也存在于本书所介绍的学习方法中。
本章的其余部分将开发此视图，但有两个基本思想：
（1）所有状态空间规划方法都将计算价值函数作为改进策略的关键中间步骤，
（2）它们通过应用于模拟经验的更新或备份操作计算价值函数。
这种通用结构可以表示如下：

.. math::

    \text{模型} \longrightarrow \text{模拟经验} \stackrel{备份}{longrightarrow} \text{价值} \longrightarrow \text{策略}

动态规划方法显然适合这种结构：它们扫描状态空间，为每个状态生成可能的转换分布。
然后，每个分布用于计算备份值（更新目标）并更新状态的估计值。
在本章中，我们认为各种其他状态空间规划方法也适合这种结构，
各种方法的区别仅在于它们的更新类型，它们的执行顺序以及备份信息的保留时间。

以这种方式查看规划方法强调了它们与我们在本书中描述的学习方法的关系。
学习和规划方法的核心是通过备份更新操作来估计价值函数。
不同之处在于，规划使用模型生成的模拟经验，而学习方法则使用环境产生的实际经验。
当然，这种差异导致了许多其他差异，例如，如何评估性能以及如何灵活地产生经验。
但是，共同的结构意味着许多想法和算法可以在规划和学习之间转移。
特别地，在许多情况下，学习算法可以代替规划方法的关键更新步骤。
学习方法仅需要经验作为输入，并且在许多情况下，它们可以应用于模拟经验以及真实经验。
下面的框显示了基于一步表格Q-learning的规划方法和来自样本模型的随机样本的简单示例。
这种方法，我们称之为 *随机样本一步表格Q-planning*，
在一步表格Q-learning收敛于真实环境的最优政策的相同条件下，收敛到模型的最优策略
（必须在步骤1中无限次地选择每个状态-动作，并且 :math:`\alpha` 必须随时间适当减小）。

.. admonition:: 随机样本一步表格Q-planning
    :class: important

    一直循环：

        1. 随机选择一个状态 :math:`S\in\mathcal{S}` 和动作 :math:`A\in\mathcal{A}`

        2. 将 :math:`S, A` 发送到样本模型，并获得样本下一个奖励 :math:`R` 和样本下一个状态 :math:`S^{\prime}`

        3. 对 :math:`S,A,R,S^{\prime}` 应用一步表格Q-learning：

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

除了统一的规划和学习方法之外，本章的第二个主题是以小的渐进步骤进行规划的好处。
这使规划能够在几乎没有浪费的计算的情况下随时中断或重定向，这似乎是将规划与行为和学习模型有效混合的关键要求。
如果问题太大而无法准确解决，即使在纯粹的规划问题上，以非常小的步骤进行规划也可能是最有效的方法。


8.2 Dyna：集成规划，行动和学习
--------------------------------

当规划在线完成时，在与环境交互时，会出现许多有趣的问题。
从交互中获得的新信息可能会改变模型，从而与规划相互作用。
可能需要以某种方式定制规划过程以适应当前正在考虑或在不久的将来预期的状态或决定。
如果决策制定和模型学习都是计算密集型过程，则可能需要在它们之间划分可用的计算资源。
为了开始探索这些问题，我们在本节中介绍了Dyna-Q，这是一个集成了在线规划个体所需的主要函数的简单架构。
每个函数都以简单，几乎无关紧要的形式出现在Dyna-Q中。
在后面的章节中，我们详细介绍了实现每个函数的一些替代方法以及它们之间的交易。
目前，我们只是为了说明这些想法并激发你的直觉。

.. figure:: images/relationships-between-experience-model-values-and-policy.png
    :align: right
    :width: 250px

在规划个体中，实际经验至少有两个角色：它可以用于改进模型（使其更准确地匹配真实环境），
并且它可以用来直接改进价值函数和政策，使用前面章节中讨论的各种强化学习方法。
前者称为模型学习，后者称为 *直接强化学习* （直接RL）。
右图中总结了经验，模型，价值和策略之间可能存在的关系。
每个箭头显示影响和推测改善的关系。请注意，经验如何通过模型直接或间接地改进价值函数和策略。
后者，有时被称为 *间接强化学习*，参与规划。

直接和间接方法都有优点和缺点。间接方法通常可以更充分地利用有限的经验，从而以更少的环境互动实现更好的策略。
另一方面，直接方法更简单，并且不受模型设计中的偏差影响。
有些人认为间接方法总是优于直接方法，而另一些方法则认为直接方法是大多数人类和动物学习的原因。
心理学和人工智能方面的相关辩论关心认知的相对重要性而不是试错学习，以及审议规划而不是被动决策
（参见第14章，从心理学的角度讨论其中的一些问题）。
我们的观点是，所有这些辩论中的替代方案之间的对比被夸大了，通过认识到这两方之间的相似性而不是反对它们，可以获得更多的洞察力。
例如，在本书中，我们强调了动态规划和时序差分方法之间的深刻相似性，即使一个是为规划而另一个是为不基于模型学习而设计的。

Dyna-Q包括上图中显示的所有过程──规划，行为，模型学习和直接RL──所有持续发生的过程。
规划方法是上一节的随机样本一步表格Q-planning方法。直接RL方法是一步表格Q-learning。
模型学习方法也是基于表格的，并假设环境是确定性的。
每次转换 :math:`S_{t}, A_{t} \rightarrow R_{t+1}, S_{t+1}` 之后，
模型在表格条目中记录 :math:`S_{t}, A_{t}` 的预测，:math:`S_{t+1}, A_{t+1}` 将确定性地跟随。
因此，如果使用之前经历过的状态-动作对查询模型，则它仅返回最后观察到的下一状态和下一个奖励作为其预测。
在规划期间，Q-planning算法仅从先前已经经历过的状态-动作对中随机采样（在步骤1中），
因此永远不会使用它没有信息的状态-动作对来查询模型。

Dyna个体的整体架构，其中Dyna-Q算法就是一个例子，如图8.1所示。
中间栏代表了个体与环境之间的基本互动，从而产生了真实经验的轨迹。
图左侧的箭头表示根据实际经验进行的直接强化学习，以改善价值函数和策略。
右边是基于模型的流程。该模型从实际经验中学习，并产生模拟经验。
我们使用术语 *搜索控件* 来指代为模型生成的模拟经验选择起始状态和动作的过程。
最后，通过将强化学习方法应用于模拟经验来实现规划，就像它们确实发生过一样。
通常，如在Dyna-Q中，相同的强化学习方法既用于从真实经验中学习，也用于从模拟经验中进行规划。
因此，强化学习方法是学习和规划的“最终共同路径”。
学习和规划是深深融合的，因为他们分享几乎所有相同的机制，只区别于他们的经验来源。

.. figure:: images/figure-8.1.png

    **图8.1：** 一般Dyna架构。在环境和策略之间来回传递的真实经验会影响策略和价值函数，
    其方式与环境模型生成的模拟经验非常相似。

从概念上讲，规划，行动，模型学习和直接RL在Dyna个体中同时并行地发生。
但是，为了在串行计算机上具体化和实现，我们完全指定它们在一个时间步长内发生的顺序。
在Dyna-Q中，行动，模型学习和直接RL过程几乎不需要计算，我们假设它们只消耗了一小部分时间。
每个步骤中的剩余时间可用于规划过程，这本身就是计算密集型过程。
我们假设在行动，模型学习和直接RL之后的每个步骤中有时间来完成Q-plainning算法的n次迭代（步骤1-3）。
在下面框中的Dyna-Q的伪代码算法中，
:math:`Model(s,a)` 表示状态-动作对 :math:`(s,a)` 的（预测的下一状态和奖励）的内容。
直接强化学习，模型学习和规划分别通过步骤（d），（e）和（f）实施。如果省略（e）和（f），则剩余算法将是一步表格Q-learning。

.. admonition:: 表格Dyna-Q
    :class: important

    对 :math:`s\in\mathcal{S}` 和 :math:`a\in\mathcal{A}`，初始化 :math:`Q(s,a)` 和 :math:`Model(s,a)`

    一直循环：

        （a） :math:`S \leftarrow` 当前（非终端）状态

        （b） :math:`A \leftarrow \varepsilon\text{-贪婪}(S, Q)`

        （c）执行动作 :math:`A`；观察结果奖励 :math:`R` 和状态 :math:`S^{\prime}`

        （d） :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

        （e） :math:`Model(S, A) \leftarrow R, S^{\prime}` （假设确定性环境）

        （f）循环 :math:`n` 次：

            :math:`S \leftarrow` 随机先前观察到的状态

            :math:`A \leftarrow` 先前在 :math:`S` 中采取的随机动作

            :math:`R, S^{\prime} \leftarrow Model(S, A)`

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

**例8.1：Dyna迷宫** 考虑图8.2中的简单迷宫。在47个状态中的每个状态中，有四个动作，**上**，**下**，**右** 和 **左**，
它们将个体确定性地带到相应的相邻状态，除非移动被障碍物或迷宫的边缘阻挡，在这种情况下个体仍然在那里。
所有过渡的奖励都是零，除了那些进入目标状态的过程，奖励是 :math:`+1`。
在达到目标状态（**G**）之后，个体返回到开始状态（**S**）以开始新的回合。
这是一个折扣的，偶然的任务，:math:`\gamma=0.95`。

图8.2的主要部分显示了将Dyna-Q个体应用于迷宫任务的实验的平均学习曲线。
初始动作价值为零，步长参数为 :math:`\alpha=0.1`，探测参数为 :math:`\varepsilon=0.1`。
当在动作中贪婪地选择时，关系（ties）随机被破坏。个体的规划步骤 :math:`n` 数量各不相同，他们按每个实际步骤执行。
对于每个 :math:`n`，曲线显示个体在每一回合中达到目标所采取的步数，对超过30次重复实验平均。
在每次重复中，随机数发生器的初始种子被保持。
因此，第一回合对于 :math:`n` 的所有值都是完全相同的（约1700步），并且其数据未在图中显示。
在第一回合之后，所有 :math:`n` 值的性能都得到改善，但是对于更大的值，性能改善更快速。
回想一下 :math:`n=0` 个体是一个非规划个体，只使用直接强化学习（一步表格式Q-learning）。
这是迄今为止这个问题上最慢的个体，尽管事实上参数值（:math:`\alpha` 和 :math:`\varepsilon`）为它进行了优化。
非规划个体花了大约25回合来达到（:math:`\varepsilon` -）最佳表现，
而 :math:`n=5` 的代理人大约需要5个回合，而 :math:`n=50` 的代理人只花了3个回合。

.. figure:: images/figure-8.2.png

    **图8.2：** 一个简单的迷宫（插图）和Dyna-Q个体的平均学习曲线，每个实际步骤的规划步骤数（:math:`n`）不同。
    任务是尽快从 **S** 旅行到 **G**。

图8.3显示了规划个体发现解决方案的速度比非规划个体快得多的原因。
显示了在第二回合中途由 :math:`n=0` 和 :math:`n=50` 个体发现的策略。
如果没有规划（:math:`n=0`），每回合只会为策略添加一个额外的步骤，因此到目前为止只学习了一步（最后一步）。
通过规划，在第一回合中再次只学习一步，但是在第二回合中，已经开发了一个广泛的策略，在该回合结束时将几乎回到开始状态。
此策略由规划过程构建，而个体仍然在启动状态附近徘徊。到第三回合结束时，将找到完整的最优政策并获得完美的表现。

.. figure:: images/figure-8.3.png

    **图8.3：** 在第二回合中途通过规划和非规划Dyna-Q个体找到的策略。
    箭头表示每个状态的贪婪行为；如果没有显示状态的箭头，则其所有动作价值都相等。
    黑色方块表示个体的位置。

在Dyna-Q中，学习和规划是通过完全相同的算法完成的，根据真实的学习经验和模拟的规划经验进行操作。
由于规划逐步进行，因此混合规划和行动是微不足道的。两者都尽可能快地进行。
个体总是被动并始终是审慎的，立即响应最新的感官信息，但总是在后台进行规划。后台中还有模型学习过程。
随着新信息的获得，模型会更新以更好地匹配现实。
随着模型的变化，正在进行的规划过程将逐渐计算出与新模型匹配的不同行为方式。

*练习8.1* 图8.3中的非规划方法看起来特别差，因为它是一步法；使用多步引导的方法会做得更好。
你认为第7章中的多步引导方法之一可以和Dyna方法一样吗？解释为什么能或者为什么不能。


8.3 当模型错误时
--------------------

在上一节中介绍的迷宫示例中，模型中的更改相对适中。模型开始是空的，然后只填充完全正确的信息。
一般来说，我们不能指望如此幸运。模型可能是不正确的，因为环境是随机的，
并且只观察到有限数量的样本，或者因为模型是使用不完全推广的函数逼近来学习的，
或者仅仅是因为环境已经改变并且尚未观察到其新的行为。当模型不正确时，规划过程可能会计算次优策略。

在某些情况下，通过规划快速计算的次优策略会导致建模错误的发现和纠正。
当模型在预测更大的奖励或更好的状态转换的意义上比实际可能更容易发生这种情况。
规划的策略试图利用这些机会并在此过程中发现它们不存在。

**例8.2：阻塞迷宫** 图8.4显示了一个迷宫示例，说明了这种相对较小的建模错误并从中恢复。
最初，从开始到目标，到屏障右侧有一条短路径，如图的左上角所示。
在1000个时间步之后，短路径被“阻挡”，并且沿着屏障的左侧打开更长的路径，如图的右上方所示。
该图显示了Dyna-Q个体和增强型Dyna-Q+个体的平均累积奖励，将在稍后描述。
图表的第一部分显示两个Dyna个体都在1000步内找到了短路径。
当环境发生变化时，图表变得平坦，这是个体没有获得奖励的时期，因为他们在屏障后面徘徊。
然而，过了一段时间，他们能够找到新的开口和新的最佳行为。

当环境变得比以前变得 *更好* 时，会出现更大的困难，但以前正确的策略并未显示出改善。
在这些情况下，如果有的话，可能无法长时间检测到建模错误。

.. figure:: images/figure-8.4.png

    **图8.4：** Dyna个体在阻塞任务上的平均性能。左侧环境用于前1000个步骤，右侧环境用于剩下的步骤。
    Dyna-Q+是带有鼓励探索的探索奖金的Dyna-Q。

**例8.3：捷径迷宫** 由这种环境变化引起的问题由图8.5中所示的迷宫示例说明。
最初，最佳路径是绕过障碍物的左侧（左上角）。然而，在3000步之后，沿着右侧打开较短的路径，而不会干扰较长的路径（右上方）。
该图显示常规Dyna-Q个体从未切换到捷径。事实上，它从未意识到它存在。
它的模型表示没有捷径，所以规划得越多，向右走并发现它的可能性就越小。
即使采用 :math:`\varepsilon` -贪婪策略，个体也不太可能采取如此多的探索性行动来发现捷径。

.. figure:: images/figure-8.5.png
    :width: 400px

    **图8.5：** Dyna个体在捷径任务上的平均性能。左边的环境用于前3000个步骤，右边的环境用于剩下的步骤。

这里的一般问题是探索和利用之间冲突的另一种形式。在规划环境中，探索意味着尝试改进模型的行动，
而利用意味着在给定当前模型的情况下以最佳方式行事。
我们希望个体进行探索以查找环境中的更改，但不要太多，以至于性能会大大降低。
与早期的探索/利用冲突一样，可能没有完美和实用的解决方案，但简单的启发式方法往往是有效的。

解决了捷径迷宫的Dyna-Q+个体使用了一种这样的启发式方法。
此个体程序会跟踪每个状态-动作对自上次尝试与环境进行真正交互以来已经经过的时间步长。
经过的时间越长，这一对的动态变化的可能性就越大（我们可以推测），并且它的模型是不正确的。
为了鼓励测试长期未尝试的行为，对涉及这些行为的模拟经验给出了特殊的“奖金奖励”。
特别是，如果转换的建模奖励是 :math:`r`，并且没有在 :math:`\tau` 时间步骤中尝试转换，
那么计划更新就好像转换产生 :math:`r+\kappa\sqrt{\tau}` 的奖励，对于一些小 :math:`\kappa`。
这样可以鼓励个体继续测试所有可访问的状态转换，甚至可以找到很长的动作序列，以便进行测试 [1]_。
当然，所有这些测试都有其成本，但在很多情况下，就像在捷径迷宫中一样，这种计算的好奇心是非常值得额外的探索。

*练习8.2* 为什么具有探索奖励Dyna-Q+的Dyna个体在第一阶段以及阻塞和捷径实验的第二阶段表现更好？

*练习8.3* 仔细检查图8.5可以发现Dyna-Q+和Dyna-Q之间的差异在实验的第一部分略有缩小。这是什么原因？

*练习8.4（编程）* 上述探索奖励实际上改变了状态和动作的估计值。这有必要吗？
假设奖金 :math:`\kappa\sqrt{\tau}` 不是用于更新，而是仅用于行动中。
也就是说，假设所选择的动作始终是 :math:`Q(S_{t}, a)+\kappa \sqrt{\tau(S_{t}, a)}` 最大的动作。
进行一个网格世界实验，测试并说明这种替代方法的优点和缺点。

*练习8.5* 如何修改8.2节上显示的表格Dyna-Q算法来处理随机环境？这种修改如何在不断变化的环境中表现不佳，如本节所述？
如何修改算法以处理随机环境 *和* 变化的环境？


8.4 优先扫描
------------------


8.5 预期与样本更新
---------------------


8.6 轨迹采样
-----------------


8.7 实时动态规划
---------------------


8.8 决策时规划
-----------------


8.9 启发式搜索
----------------


8.10 推出（Rollout）算法
------------------------------


8.11 蒙特卡洛树搜索
---------------------


8.12 本章总结
---------------


8.13 第一部分摘要：维度（Dimensions）
----------------------------------------


书目和历史评论
---------------

.. [1]
    Dyna-Q+个体也以另外两种方式改变。首先，允许在之前框中的表格Dyna-Q算法的规划步骤（f）中
    考虑从一个状态以前从未尝试过的动作。其次，这种行为的初始模型是，他们会以零回报率回到同一个状态。
