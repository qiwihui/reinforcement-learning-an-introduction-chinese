第3章 有限马尔科夫决策过程
==========================

在本章中，我们将介绍有限马尔可夫决策过程或有限MDP的形式问题，我们将在本书的其余部分尝试解决这些问题。
这个问题涉及评价反馈，如在老虎机问题中，但也涉及一个关联方面，即在不同情况下选择不同的行动。
MDP是顺序决策的经典形式化，其中行动不仅影响直接奖励，还影响后续情况或状态，以及贯穿未来的奖励。
因此，MDP涉及延迟奖励以及交换即时与延迟奖励的需要。
在老虎机问题中，我们估计每个动作a的价值 :math:`q_*(a)`，
在MDP中我们估计每个状态s中每个动作a的价值 :math:`q_*(s, a)`，
或者我们估计给出最佳行动选择的每个状态的价值 :math:`v_*(s)`。
这些依赖于状态的量对于准确地为个人行动选择的长期结果分配信用至关重要。

MDP是强化学习问题的数学理想化形式，可以对其进行精确的理论陈述。
我们介绍问题数学结构的关键元素，如回归，值函数和Bellman方程。
我们试图传达可能被用作有限MDP的各种可能的应用程序。
与所有人工智能一样，在适用范围和数学易处理性之间存在着一种矛盾。
在本章中，我们将介绍这种矛盾关系，并讨论它所暗示的一些权衡和挑战。
第17章介绍了在MDP之外进行强化学习的一些方法。

3.1 个体环境接口
^^^^^^^^^^^^^^^^^^
​
MDP旨在直接构建从交互中学习以实现目标的问题。
学习者和决策者被称为 *个体（agent）*。它与之交互的东西，包括个体之外的所有东西，被称为 *环境*。
这些交互持续不断，个体选择动作同时环境响应那些动作并向个体呈现新情况 [1]_。
环境还产生奖励，这是个体通过其行动选择寻求最大化的特殊数值。

.. figure:: images/figure-3.1.png

    图3.1：马尔可夫决策过程中的个体 - 环境交互。

更具体地，个体和环境在离散时间序列每一步相互作用，:math:`t = 0,1,2,3,\dots` [2]_。
在每个时间步t，个体接收环境 *状态* :math:`S_{t} \in \mathcal{S}` ，
并在此基础上选择一个 *动作*，:math:`A_{t}\in \mathcal{S}(s)` [3]_。
每一步后，作为它行动的结果，个体接收到一个 *奖励* 值，
:math:`R_{t+1} \in \mathcal{R} \subset \mathbb{R}`，
并且自身处于一个新状态，:math:`S_{t+1}` [4]_ ，
MDP和个体一起产生了一个如下所示的序列或 *轨迹*：

.. math::
    :label: 3.1

    S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\dots

在 *有限* MDP中，状态，动作和奖励
（:math:`\mathcal{S}`，:math:`\mathcal{A}` 和 :math:`\mathcal{R}`）的集合都具有有限数量的元素。
在这种情况下，随机变量 :math:`R_t` 和 :math:`S_t` 具有明确定义的离散概率分布，仅取决于先前的状态和动作。
也就是说，对于这些随机变量的特定值，:math:`s' \in \mathcal{S}` 和 :math:`r \in \mathcal{R}`，
在给定前一状态和动作的特定值的情况下，存在这些值在时间t发生的概率：

.. math::
    :label: 3.2

    p(s',r|s,a) \doteq Pr\{S_t=s',R_t=r|S_{t-1}=s,R_{t-1}=a\}

对所有 :math:`s', s \in \mathcal{S}`，:math:`\mathcal{r} \in \mathcal{R}`
和 :math:`a \in \mathcal{A}(s)`。函数 :math:`p` 定义了MDP的 *动态*。
方程中等号上的点提醒我们它是一个定义（在这个例子中是函数 :math:`p`），而不是从先前定义得出的事实。
动力学函数 :math:`p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1]`
是四个参数的普通确定性函数。 中间的“|”来自条件概率的符号，
但这里只是提醒我们 :math:`p` 指定 :math:`s` 和 :math:`a` 的每个选择的概率分布，即

.. math::
    :label: 3.3

    \sum_{s' \in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s',r|s,a)=1，对所有 s \in \mathcal{S}，a \in \mathcal{A}(s)

在 *马尔可夫* 决策过程中，:math:`p` 给出的概率完全表征了环境的动态。
也就是说，:math:`S_t` 和 :math:`R_t` 的每个可能值的概率
仅取决于前一个状态和动作 :math:`S_{t-1}` 和 :math:`A_{t-1}`，
并且在给定它们的情况下，它们根本不依赖于先前的状态和动作。
最好将其视为对决策过程的限制，而不是对 *状态* 的限制。
状态必须包括有关过去的个体-环境交互的所有方面的信息，这些信息对未来有所影响。
如果确实如此，那么就说该状态拥有 *马尔可夫性*。我们将在本书中假设马尔可夫性，
尽管从第二部分开始我们将考虑不依赖它的近似方法，并且在第17章中我们考虑如何从非马尔可夫观察中学习和构造马尔可夫状态。

从四参数动力学函数p中，可以计算出人们可能想知道的关于环境的任何其他信息，例如状态转移概率（我们将其略微滥用符号表示为三参数函数
:math:`p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, 1]`），

.. math::
    :label: 3.4

    p(s'|s,a) \doteq Pr\{S_t=s'|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)

我们还可以将状态 - 动作对的预期奖励计算为双参数函数 :math:`r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}`，

.. math::
    :label: 3.5

    r(s,a)\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)

以及状态 - 行动 - 下一状态三元组的预期奖励作为三个参数函数
:math:`r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}`，

.. math::
    :label: 3.6

    r(s,a,s')\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}

在本书中，我们通常使用四参数p函数（3.2），但这些其他符号中的每一个偶尔也很方便。

MDP框架是抽象和灵活的，可以以不同的方式应用在很多不同的问题上。
例如，时间步长不需要指固定的实时间隔；它们可以指任意连续的决策和行动阶段。
这些动作可以是低级控制，例如施加到机器人手臂的电动机的电压，或高级​​决策，例如是否要吃午餐或进入研究生院。
同样，状态也可以采取各种各样的形式。它们可以完全由低级感觉决定，例如直接传感器读数，
或者它们可以更高级和抽象，例如房间中物体的符号描述。
可以基于对过去的感觉的记忆，甚至是完全精神的或主观的来构成一个状态。
例如，个体可能处于不确定对象在哪里的状态，或者在某些明确定义的意义上感到惊讶的状态。
同样，某些行为可能完全是精神上的或可计算的。例如，某些操作可能会控制代理选择考虑的内容，或者它关注的重点。
一般而言，行动可以是我们想要学习如何制定的任何决定，而状态可以是我们可以知道的任何可能有助于制作它们的任何事物。

特别是，个体和环境之间的边界通常与机器人或动物身体的物理边界不同。
通常，边界更接近于个体。例如，机器人及其传感硬件的电动机和机械联动件通常应被视为环境的一部分而不是个体的一部分。
同样，如果我们将MDP框架应用于人或动物，肌肉，骨骼和感觉器官应被视为环境的一部分。
也许，奖励可以在自然和人工学习系统的物理体内计算，但被认为是个体的外部。​

我们遵循的一般规则是，任何不能被个体任意改变的东西都被认为是在它之外，因此也是其环境的一部分。
我们不假定个体对环境一无所知。例如，个体通常非常了解如何根据其动作及其所处的状态来计算奖励。
但是我们总是认为奖励计算是在个体之外的，因为它是根据个体的任务所定义的，因此不能由个体来随意改变。
事实上，在某些情况下，个体就算知道它的环境是如何运行的，并且仍然面临着艰难的强化学习任务，
正如我们可以知道一个魔方是如何运行的，但仍然无法解开它。
个体-环境的边界代表着对个体的绝对控制能力的限制，而不是限制它的知识。

个体-环境的边界可以位于不同的地方以用于不同的目的。在复杂的机器人中，许多不同的个体可能同时运行，每个个体都有自己的边界。
例如，一个个体可以做出高级决策，高级决策可由低级个体面临的状态组成，从而实现高层次的决策。
在实践中，一旦选择了特定的状态，动作和奖励，就确定个体-环境边界，从而确定了感兴趣的特定决策制定任务。

MDP框架是从相互作用的目标导向学习的问题中抽象出来的。
它提出无论传感，记忆和控制装置的细节，以及任何目标试图达到的目标，
学习目标导向行为的任何问题都可以减少为个体及其环境之间来回传递的三个信号：
一个信号表示个体做出的选择（动作），一个信号表示作出选择的基础（状态），以及另一个信号来定义个体的目标（奖励）。
这个框架可能不足以有效地代表所有决策学习问题，但它已被证明是广泛有用和适用的。

当然，特定的状态和操作因任务而异，并且它们的表示方式会对性能产生很大影响。
在强化学习中，与其他类型的学习一样，这种表征性选择目前更多的是艺术而非科学。
在本书中，我们提供了一些关于表达状态和行为的好方法的建议和例子，但我们主要焦点的是一旦表示被确定，如何学习行为的一般原则。

**例3.1：生物反应器** 假设强化学习用于确定生物反应器（用于生产有用化学品的大量营养物和细菌）的瞬间温度温度和搅拌速率。
这种应用中的动作可以是传递到下级控制系统的目标温度和目标搅拌速率，该控制系统又直接激活加热元件和马达以实现目标。
状态可能是有可能被过滤和延迟热电偶和其他传感器读数，加上代表大桶和目标化学品成分的符号输入。
奖励可能是生物反应器产生有用化学品的速率的逐时测量。
请注意，此处每个状态都是传感器读数和符号输入的列表或矢量，每个动作都是由目标温度和搅拌速率组成的矢量。
强化学习任务的典型特征是具有这种结构化表示的状态和动作。另一方面，奖励总是单个数字。

**例3.2：拾取和放置机器人** 考虑使用强化学习来控制机器人手臂在重复拾取和放置任务中的运动。
如果我们想要学习快速和平稳的运动，则当前个体将必须直接控制马达并且具有关于机械联动装置的当前位置和速度的低延迟信息。
在这种情况下的动作可能是每个关节处施加到每个电动机的电压，并且状态可能是关节角度和速度的最新读数。
对于成功拾取和放置的每个对象，奖励可能为+1。为了鼓励平稳移动，在每个时间步骤上，可以根据动作的瞬间“急动”给出小的负面奖励。

*练习3.1* 设计适合MDP框架的三个自己的示例任务，为每个任务确定其状态，动作和奖励。
尽可能使这三个例子彼此 *不同*。该框架是抽象和灵活的，可以以多种不同的方式应用。在至少一个示例中以某种方式扩展其限制。

*练习3.2* MDP框架是否足以有效地代表 *所有* 目标导向的学习任务？你能想到任何明显的例外吗？

*练习3.3* 考虑驾驶问题。你可以根据加速器，方向盘和制动器（即你的身体与机器接触的位置）来定义动作。
或者你可以将它们定义得更远，比如橡胶与道路相遇，考虑你的动作是轮胎扭矩。
或者你可以进一步定义它们，比如说，你的大脑掌控身体，肌肉抽搐的动作来控制你的四肢。
或者你可以达到一个更高的层次，说你的行动是你选择开车的地方。
什么是个体和环境之间合适的层次和位置分界？在什么基础上，该线的一个位置优先于另一个？
是否有任何根本原因选择一个位置而不是另一个位置，还是随意选择？

.. admonition:: 例3.3：环保机器人
    :class: important

    移动机器人的工作是在办公室环境中收集空的汽水罐。它有用于检测汽水罐的传感器，以及可以将它们拾起并放置在机箱中的臂和夹具；它使用可充电电池供电。
    机器人的控制系统具有用于解释传感器信息，用于导航以及用于控制手臂和夹具的部件。
    关于如何搜索汽水罐的高级决策是由强化学习个体根据电池的当前充电水平做出的。
    举一个简单的例子，我们假设只能区分两个电荷电平，包括一个小的状态集 :math:`\mathcal{S}=\{高，低\}`。
    在每个状态，个体可以决定是否（1）在一段时间内主动 **搜索** 汽水罐，（2）保持静止并 **等待** 某人给它汽水罐，或（3）返回其本垒为电池 **充电**。
    当能量水平很 **高** 时，充电总是愚蠢的，所以我们不会将其包含在为此状态设定的动作中。
    动作集是 :math:`\mathcal{A}(高)=\{搜索, 等待\}` 和 :math:`\mathcal{A}(低)=\{搜索, 等待, 充电\}`。

    奖励在大多数情况下为零，但是当机器人固定空罐时变为正值，或者如果电池完全耗尽则变为负值。
    找到汽水罐的最好方法是主动搜索它们，但这会耗尽机器人的电池电量，而等待则不会。
    每当机器人正在搜索时，存在其电池耗尽的可能性。在这种情况下，机器人必须关闭并等待获救（产生低回报）。
    如果电池电量水平 **高**，则可以始终完成一段主动搜索而没有耗尽电池的风险。
    以 **高** 电量水平开始的搜索周期使电量水平以概率 :math:`\alpha` 保持并且以概率 :math:`1-\alpha` 降低至 **低** 电量水平。
    另一方面，当电量水平 **低** 时进行的搜索周期使其以概率 :math:`\beta` 变 **低** 并且以概率 :math:`1-\beta` 消耗电池。
    在后一种情况下，必须拯救机器人，然后将电池重新充电至 **高** 电量水平。
    机器人收集的每个汽水罐都可以作为单位奖励计算，而每当机器人必须获救时，奖励为-3。
    用 :math:`r_{搜索}` 和 :math:`r_{等待}`，其中 :math:`r_{搜索}>r_{等待}`，分别表示机器人在搜索和等待时将收集的预期罐数（以及预期的奖励）。
    最后，假设在跑步回家期间不能收集罐头，并且在电池耗尽的过程中不能收集罐头。
    这个系统是一个有限的MDP，我们可以记下转移概率和预期的奖励，动态如左表所示：

    .. figure:: images/table-figure.png

    请注意，表中有一行代表当前状态 :math:`s`，动作 :math:`a`，:math:`a\in\mathcal{A}(s)`和下一个状态 :math:`s` 的每种可能组合。
    某些转换的概率为零，因此没有为它们指定预期的奖励。右侧所示是另一种有用的方法，可以总结有限MDP的动态，称为 *转换图*。
    有两种节点：*状态节点* 和 *动作节点*。每个可能的状态都有一个状态节点（由状态名称标记的大圆圈），
    以及每个状态-动作对的动作节点（由行动名称标记并由线连接的小实心圆圈）。
    从状态 :math:`s` 开始并采取动作 :math:`a`，你将沿着从状态节点 :math:`s` 到动作节点 :math:`(s,a)` 的线路移动。
    然后，环境通过离开动作节点 :math:`(s,a)` 的箭头之一转换到下一个状态的节点。
    每个箭头对应一个三元组 :math:`(s,s',a)`，其中 :math:`s'` 是下一个状态，我们用转移概率 :math:`p(s'|s,a)` 标记箭头，
    以及该转换的预期回报 :math:`r(s,a,s')`。请注意，标记离开动作节点的箭头的转移概率和总是为1。

*练习3.4* 给出一个类似于例3.3中的表，但是对于 :math:`p(s',r|s,a)`。
它应该有 :math:`s, a, s', r` 和 :math:`p(s',r|s,a)` 的列，
以及 :math:`p(s',r|s,a)>0` 的每个4元组的行。

3.2 目标和奖励
^^^^^^^^^^^^^^



.. [1]
   我们使用术语个体，环境和动作，而不是工程师术语控制器，受控系统（或工厂）和控制信号，因为它们对更广泛的受众有意义。

.. [2]
   我们将注意力限制在离散时间以使事情尽可能简单，即使许多想法可以延伸到连续时间情况
   （例如，参见Bertsekas和Tsitsiklis，1996；Werbos，1992；Doya，1996）。

.. [3]
   我们使用 :math:`R_{t+1}` 而不是 :math:`R_{t}` 来表示归因于 :math:`A_{t}` 的奖励，
   因为它强调下一个奖励和下一个状态 :math:`R_{t+1}` 和 :math:`S_{t+1}` \ 共同确定。
   不幸的是，这两种惯例在文献中都被广泛使用。

.. [4]
   更好的方式是传授这种先验知识是最初的策略或价值功能，或对这些的影响。
   参见Lin（1992），Maclin和Shavlik（1994）和Clouse（1996）。

.. [5]
   情节有时在文献中称为“试验”。
