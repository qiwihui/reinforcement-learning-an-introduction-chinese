第15章 神经科学
====================

神经科学是对神经系统的多学科研究的总称，主要包括：如何调节身体功能，如何控制行为，
由发育、学习和老化所引起的随着时间的变化，以及细胞和分子机制如何使这些功能成为可能。
强化学习的最令人兴奋的方面之一是来自神经科学的越来越多的证据表明，
人类和许多其他动物的神经系统实施的算法和强化学习算法在很多方面是一一对应的。
本章主要解释这些相似之处，以及他们对动物的基于收益的学习的神经基础的看法。

强化学习和神经科学之间最显著的联系就是多巴胺，它是一种哺乳动物大脑中与收益处理机制紧密相关的化学物质。
多巴胺的作用就是将TD误差传达给进行学习和决策的大脑结构。
这种相似的关系被表示为 *多巴胺神经元活动的收益预测误差假说*，这是由强化学习和神经科学实验结果引出的一个假设。
在本章中我们将讨论这个假设，引出这个假设的神经科学发现，以及为什么它对理解大脑收益系统有重要作用。
我们还会讨论强化学习和神经科学之间的相似之处，虽然这种相似不如多巴胺／TD误差之间的相似那么明显，
但它提供了有用的概念工具，用于研究动物的基于收益的学习机制。
强化学习的其他元素也有可能会影响神经系统的研究，但是本章对它们与神经科学之间的联系相对讨论得不多。
我们在本章只讨论一些我们认为随着时间的推移会变得重要的联系。

正如我们在本书第1章的强化学习的早期历史部分（1.7节）所概述的，强化学习的许多方面都受到神经科学的影响。
本章的第二个目标是向读者介绍有关脑功能的观点，这些观点对强化学习方法有所贡献。
从脑功能的理论来看，强化学习的一些元素更容易理解。
对于“资格迹”这一概念尤其如此，资格迹是强化学习的基本机制之一起源于突触的一个猜想性质（突触是神经细胞与神经元之间相互沟通的结构）。

在本章，我们并没有深入研究动物的基于收益学习的复杂神经系统，因为我们不是神经科学家。
我们并不试图描述（甚至没有提及）许多大脑结构，或任何分子机制，即使它们都被认为参与了这些过程。
我们也不会对与强化学习非常吻合的假设和模型做出评判。神经科学领域的专家之间有不同的看法是很正常的。
我们仅仅想给读者讲好有吸引力和建设性的例子。
我们希望这一章给读者展现多种将强化学习及其理论基础与动物的基于收益学习的神经科学理论联系起来的渠道。

许多优秀的著作介绍了强化学习与神经科学之间的联系，我们在本章的最后一节中引用了其中的一些。
我们的方法和这些方法不太相似因为我们假设读者熟悉本书前面几章所介绍的强化学习，但是不了解有关神经科学的知识。
因此我们首先简要介绍神经科学的概念，以便让你有基本的理解。


15.1 神经科学基础
------------------

了解一些关于神经系统的基本知识有助于理解本章的内容。我们后面提到的术语用楷体表示。
如果你已经有神经科学方面的基本知识，则可以跳过这一节。

*神经元* 是神经系统的主要组成部分，是专门用于电子和化学信号的处理及信息传输的细胞。
它们以多种形式出现，但神经元通常具有细胞体、*树突* 和单个 *轴突*。
树突是从细胞体分叉出来，以接收来自其他神经元的输入（或者在感觉神经元的情况下还接收外部信号）的结构。
神经元的轴突是将神经元的输出传递给其他神经元（或肌肉、腺体）的纤维。
神经元的输出由被称为 *动作电位* 的电脉冲序列构成，这些电脉冲沿着轴突传播。
动作电位也被称为 *尖峰*，而神经元在产生尖峰时被认为是触发的。
在神经网络模型中，通常使用实数来表示神经元的 *放电速率*，即每单位时间的平均放电次数。

神经元的轴突可以分很多叉，使神经元的动作电位达到许多目标。神经元轴突的分叉结构部分被称为神经元的 *轴突中枢*。
因为动作电位的传导是一个主动过程，与导火索的燃烧不同，所以当动作电位到达轴突的分叉点时，
它会“点亮”所有输出分支上的动作电位（尽管有时会无法传播到某个分支）。
因此，具有大型轴突中枢的神经元的活动可以影响许多目标位置。

*突触* 通常是轴突分叉终止处的结构，作为中介调整一个神经元与另一个神经元之间的通信。
突触将信息从 *突触前* 神经元的轴突传递到 *突触后* 神经元的树突或细胞体。
除少数例外，当动作电位从突触前神经元传输到突触的时候突触会释放化学 *神经递质*
（但有时神经元之间有直接电耦合的情况，但是在这里我们不涉及这些）从突触的前侧释放的神经递质分子会弥漫在 *突触间隙*，
即突触前侧的末端和突触后神经元之间的非常小的空间，
然后与突触后神经元表面的受体结合，以激发或抑制其产生尖峰的活性，或以其他方式调节其行为。
一种特定的神经递质可能与几种不同类型的受体结合，每种受体在突触后神经元上产生不同的反应。
例如，神经递质多巴胺至少可以通过五种不同类型的受体来影响突触后神经元。
许多不同的化学物质已被确定为动物神经系统中的神经递质。

神经元的 *背景* 活动指的是“背景”情况下的活动水平，通常是它的放电速率。
所谓“背景情况”是指神经元的活动不是由实验者指定的任务相关的突触输入所驱动的，
例如，当神经元的活动与作为实验的一部分传递给被试者的刺激无关时，我们就认为其活动是背景活动。
背景活动可能由于输入来自于更广泛的网络而具有不规则性，或者由于神经或突触内的噪声而显得不规则。
有时背景活动是神经元固有的动态过程的结果。与其背景活动相反，神经元的 *阶段性* 活动通常由突触输入引起的尖峰活动冲击组成。
对于那些变化缓慢、经常以分级的方式进行的活动，无论是否是背景活动，都被称为神经元的 *增补* 活动。

突触释放的神经递质对突触后神经元产生影响的强度或有效性就是突触的 *效能*。
一种利用经验改变神经系统的方式就是通过改变突触的效能来改变神经系统，
这个“效能”是突触前和突触后神经元的活动的组合产生的结果，有时也来自于神经调节剂产生的结果。
所谓 *神经调节剂*，就是除了实现直接的快速兴奋或抑制之外，还会产生其他影响的神经递质。

大脑含有几个不同的神经调节系统，由具有广泛分叉的树状轴突神经元集群组成，每个系统使用不同的神经递质。
神经调节可以改变神经回路的功能、中介调整的动因、唤醒、注意力、记忆、心境、情绪、睡眠和体温。
这里重要的是，神经调节系统可以分配诸如强化信号之类的标量信号以改变突触的操作，
这些突触往往广泛分布在不同地方但对神经元的学习具有关键作用。

突触效能变化的能力被称为 *突触的可塑性*。
这是学习活动的主要机制之一通过学习算法调整的参数或权重对应于突触的效能（synaptic efficacies）正如我们下面要详细描述的，
通过神经调节剂多巴胺对突触可塑性进行调节是大脑实现学习算法的一种机制，就像本书所描述的那些算法一样。


15.2 收益信号、强化信号、价值和预测误差
----------------------------------------

神经科学和计算型的强化学习之间的联系始于大脑信号和在强化学习理论与算法中起重要作用的信号之间的相似性。
在第3章中，我们提到，任何对目标导向的行为进行学习的问题描述都可以归结为具有代表性的三种信号：动作、状态和收益。
然而，为了解释神经科学和强化学习之间的联系，我们必须更加具体地考虑其他强化学习的信号，这些信号以特定的方式与大脑中的信号相对应。
除了收益信号以外，还包含强化学习信号（我们认为这些信号不同于收益信号）、价值信号和传递预测误差的信号。
当我们以某种方式用对应函数来标记一个信号的时候，我们就在强化学习理论的语境之下把信号和某个公式或算法中的一项对应起来。
另一方面，当我们提到大脑中的一个信号时，也是想表示一个生理事件，比如动作电位的突变或者神经递质的分泌。
把一个神经信号标记为对应函数，比如把一个多巴胺神经元相位活动称为一个强化信号，意味着我们推测这个神经信号的作用与强化学习理论中的信号作用类似。

找到这些对应关系的证据面临诸多挑战。与收益处理过程相关的神经活动几乎可以在大脑的每一个部分找到，
但是由于不同的信号通常具有高度相关性，因此我们很难清楚地解释结果。
我们需要设计严谨的实验来把一种类型的收益相关信号和其他类型的收益信号区别开来，或者和其他与收益过程无关的大量信号区别开来。
尽管存在这些困难，但我们已经进行了许多实验来使强化学习理论和算法与神经信号对应起来，并建立一些具有说服力的联系。
为了在后续章节中说明这些联系，在本节的后面我们将告诉读者各种收益相关的信号与强化学习理论中信号的对应关系。

在第14章末介绍术语时，我们说到的 :math:`R_{t}` 更像动物大脑中的收益信号，而非动物环境中的物体或事件。
收益信号（以及智能体的环境）定义了强化学习智能体正试图解决的问题。
就这一点而言，:math:`R_{t}` 就像动物大脑中的一个信号，定义收益在大脑各个位置的初始分布。
但是在动物的大脑中不可能存在像 :math:`R_{t}` 这样的统一的收益信号。
我们最好把 :math:`R_{t}` 看作一个概括了大脑中许多评估感知和状态奖惩性质的系统产生的大量神经信号整体效应的抽象。

强化学习中的 *强化信号* 与收益信号不同。强化信号的作用是在一个智能体的策略、价值估计或环境模型中引导学习算法做出改变。
对于时序差分方法，例如，:math:`t` 时刻的强化信号是TD误差 :math:`\delta_{t-1}=R_{t}+\gamma V(S_{t})-V(S_{t-1})` [1]_。
某些算法的强化信号可能仅仅是收益信号，但是大多数是通过其他信息调整过的收益信号，例如TD误差中的价值估计。

状态价值函数或动作价值函数的估计，即 :math:`V` 或 :math:`Q`，指明了在长期内对智能体来说什么是好的，什么是坏的。
它们是对智能体未来期望积累的总收益的预测。智能体做出好的决策，
就意味着选择合适的动作以到达具有最大估计状态价值的状态，或者直接选择具有最大估计动作价值的动作。

.. [1]
    如我们在6.1节中介绍的，在我们的符号体系下 :math:`\delta_{t}` 被定义为 :math:`R_{t+1}+\gamma V(S_{t+1})-V(S_{t})`，
    所以，只有到了 :math:`t+1` 时刻才能得到 :math:`\delta_{t}`。
    则 :math:`t` 时刻的TD误差实际是 :math:`\delta_{t-1}=R_{t}+\gamma V\left(S_{t}\right)-V\left(S_{t-1}\right)`。
    因为我们通常认为每个时间步长是非常小甚至有时可以认为是无限小的，以对于定义上面这样的单个时刻的偏移不需要过分解读它的重要性。

预测误差衡量期望和实际信号或感知之间的差异。收益预测误差（reward prediction errors，RPE）衡量期望和实际收到的收益信号之的差异，
当收益信号大于期望时为正值，否则为负值。像式（6.5）中的TD误差是特殊类的RE，它表示当前和早先的长期回报期望之间的差异。
当神经科学家提到RPE时，他们一般（但不总是）指 TD RPE，在本章中我们简单地称之为TD误差。
在本章中TD误差通常不依赖于动作，不同于在 Sarsa和Q－学习算法中学习动作价值时的TD误差。
这是因为最明显的与神经科学的联系是用动作无关的TD误差来表述的，但是这并不意味着不存在与动作相关的TD误差的联系
（用于预测收益以外信号的TD误差也是有用的，但我们不加以考虑，这类例子可以参考 Modayil、 White和 Sutton，2014）。

关于神经科学数据与这些从理论上定义的信号之间的联系，我们可以提很多问题。
比如，观测到的信号更像一个收益信号、价值信号预测误差、强化信号，还是一个完全不同的东西？
如果是误差信号，那是收益预测误差（RE）、TD误差，还是像 Rescorla－Wagner误差（式14.3）这样的更简单的误差？
如果是TD误差，那是否是动作相关的“Q学习”或 Sarsa 等误差？如上所述，通过探索大脑来回答这样的问题是非常困难的。
但实验证据表明，一种神经递质，特别是多巴胺，表示RPE信号，而且生产多巴胺的神经元的相位活动事实上会传递TD误差（见15.1节节关于相位活动的定义）。
这个证据引出了 *多巴胺神经元活动的收益预测误差假说*，我们将在下面描述。


15.3 收益预测误差假说
-----------------------

*多巴胺神经元活动的收益预测误差假说* 认为，哺乳动物体内产生多巴胺的神经元的相位活动的功能之一，
就是将未来的期望收益的新旧估计值之间的误差传递到整个大脑的所有目标区域。
Montague、 Dayan和 Sejnowski1996）首次明确提出了这个假说（虽然没有用这些确切的词语），
他们展示了强化学习中的TD误差概念是如何解释哺乳动物中多巴胺神经元相位活动各种特征的。
引出这一假说的实验于20世纪80年代、90年代初在神经科学家沃尔夫拉姆·舒尔茨的实验室进行。
15.4节描述了这些重要实验，15.6节解释了这些实验的结果与TD误差的一致性，
本章末尾的参考文献和历史评注部分包含了记录这个重要假设发展历程的文献。

Montague等人（1996）比较了经典条件反射下时序差分模型产生的TD误差和经典条件反射环境下产生多巴胺的神经元的相位活动。
回顾14.2节，经典条件反射下的时序差分模型基本上是线性函数逼近的半梯度下降TD（:math:`\lambda`）算法。
Montague等人做了几个假设来进行对比。首先，由于TD误差可能是负值，但神经元不能有负的放电速率，
所以他们假设与多巴胺神经元活动相对应的量是 :math:`\delta_{t-1}+b_{t}`，其中 :math:`b_t` 是神经元的背景放电速率。
负的TD误差对应于多巴胺神经元低于其背景放电速率的放电速率降低量 [2]_。

第二个假说是关于每次经典条件反射试验所访问到的状态以及它们作为学习算法的输入量的表示方式的。
我们在14.2.4中针对时序差分模型讨论过这个问题。
Montague等人选择了全串行复合刺激表示（CSC），如图141左边一列所示，但略有不同的是，
短期内部信号的序列一直持续到US开始出现，而这里就是非零收益信号到达的地方。
这种表示方式使得TD误差能够模仿这样一种现象：多巴胺神经元活动不仅能预测未来收益，也对收到预测线索之后，收益 *何时* 可以达成是敏感的。
我们必须有一些方法来追踪感官线索和收益达成之间的间隔时间。
如果一个刺激对其后会继续产生的内部信号的序列进行了初始化，并且它们在刺激结束之后的每个时刻都产生不同的信号，
那么在每个时刻，我们可以用不同的状态来表示这些信号。因此，依赖于状态的TD误差对试验中事件发生的时间是敏感的。

有了这些关于背景放电速率和输入表示的假说，在15.5节的模拟试验中，时序差分模型的TD误差与多巴胺神经元的相位活动就十分相似了。
在15.5节中我们对这些相似性细节进行了描述，TD误差与多巴胺神经元的下列特征是相似的：
1）多巴胺神经元的相位反应只发生在收益事件不可预测时；
2）在学习初期，在收益之前的中性线索不会引起显著的相位多巴胺反应，但是随着持续的学习，这些线索获得了预测值并随即引起了相位多巴胺反应；
3）如果存在比已经获得预测值的线索更早的可靠线索，则相位多巴胺反应将会转移到更早的线索，并停止寻找后面的线索；
4）如果经过学习之后，预测的收益事件被遗漏，则多巴胺神经元的反应在收益事件的期望时间之后不久就会降低到其基准水平之下。

虽然在 Schultz等人的实验中，并不是每一个被监测到的多巴胺神经元都有以上这些行为，
但是大多数被监测神经元的活动和TD误差之间惊人的对应关系为收益预测误差假说提供了强有力的支持。
然而，仍存在一些情况基于假设的预测与实验中观察到的不一致。
输入表示的选择对于TD误差与多巴胺神经元活动某些细节之间的匹配程度来说至关重要，特别是多巴胺神经元的反应时间的细节。
为了使二者更加吻合，有一些关于输入表示和时序差分学习其他特征的不同思想被提了出来，
我们会在下面讨论一些，但主流的表示方法还是 Montague等人的CSC表示方法。
总体而言，收益预测误差假说已经在研究收益学习的神经科学家中被广泛接受，并且已经被证明能适应来自神经科学实验的更多结果。

为描述支持收益预测误差假说的神经科学实验，我们会提供一些背景使得假设的重要性更容易被理解。
我们接下来介绍一些关于多巴胺的知识，和它影响的大脑结构，以及它们是如何参与收益学习过程的。

.. [2]
    多巴胺神经元活动相关的TD误差中的 :math:`\delta_t` 与
    我们的 :math:`\delta_{t-1}=R_{t}+\gamma V(S_{t})-V(S_{t-1})` 是类似的。


15.4 多巴胺
-----------------

多巴胺是神经元产生的一种神经递质，其细胞体主要位于哺乳动物大脑的两个神经元群中：黑质致密部（SNpc）和腹侧被盖区（VTA）。
多巴胺在哺乳动物大脑的许多活动中起着重要的作用。其中突出的是动机、学习、行动选择、大多数形式的成瘾、精神分裂症和帕金森病。
多巴胺被称为神经调节剂，因为除了直接快速使靶向神经元兴奋或抑制靶向神经元之外，多巴胺还具有许多功能。
虽然多巴胺的很多功能和细胞效应的细节我们仍不清楚，但显然它在哺乳动物大脑收益处理过程中起着基础性的作用。
多巴胺不是参与收益处理的唯一神经调节剂，其在厌恶情况下的作用（惩罚）仍然存在争议。
多巴胺也可以在非哺乳动物中发挥作用。但是在包括人类在内的哺乳动物的收益相关过程中，多巴胺起到的重要作用毋庸置疑。

一个早期的传统观点认为，多巴胺神经元会向涉及学习和动机的多个大脑区域广播收益信号。
这种观点来自詹姆斯·奥尔德斯（James Olds）和彼得米尔纳（Peter Milner）他们在1954年著名的论文中描述了电刺激对老鼠大脑某些区域的影响。
他们发现，对特定区域的电刺激对控制老鼠的行为方面有极强的作用：
“……通过这种收益对动物的行为进行控制是极有效的，可能超过了以往所有用于动物实验的收益”（olds和 Milner，1954）。
后来的研究表明，这些对最敏感的位点的刺激所激发的多巴胺通路，
通常就是直接或间接地被自然的收益刺激所激发的多巴胺通路在人类被试者中也观察到了与老鼠类似的效应。
这些观察结果有效表明多巴胺神经元活动携带了收益信息。

但是，如果收益预测误差假说是正确的，即使它只解释了多巴胺神经元活动的某些特征，
那么这种关于多巴胺神经元活动的传统观点也不完全正确：多巴胺神经元的相位反应表示了收益预测误差，而非收益本身。
在强化学习的术语中，时刻 :math:`t` 的多巴胺神经元相位反应
对应于 :math:`\delta_{t-1}=R_{t}+\gamma V(S_{t})-V(S_{t-1})`，而不是 :math:`R_t`。

强化学习的理论和算法有助于一致性地解释“收益-预测-误差”的观点与传统的信号收益的观点之间的关系。
在本书讨论的许多算法中，作为一个强化信号，:math:`\delta` 是学习的主要驱动力。
例如，:math:`\delta` 是经典条件反射时序差分模型中的关键因素，
也是在“行动器-评判器”框架中学习价值函数和策略的强化信号（13.5节和15.7节）。
:math:`\delta` 的动作相关的形式是Q学习和Sarsa的强化信号。
收益信号 :math:`R_t` 是 :math:`\delta_{t-1}` 的重要组成部分，但不是这些算法中强化效应的完全决定因素。
附加项 :math:`\gamma V(S_{t})-V(S_{t-1})` 是 :math:`\delta_{t-1}` 的次级强化部分，
即使有收益（:math:`R_{t} \neq 0`）产生，如果收益可以被完全预测，则TD误差也可以是没有任何影响的（15.6节详细解释）。

事实上，仔细研究Olds和 Milner1 1954年的论文可以发现，这主要是工具性条件反射任务中电刺激的强化效应。
电刺激不仅能激发老鼠的行为通过多巴胺对动机的作用，还导致老鼠很快学会通过按压杠杆来刺激自己，而这种刺激会长时间频繁进行。
电刺激引起的多巴胺神经元活动强化了老鼠的杠杆按压动作。

最近使用光遗传学方法的实验证实了多巴胺神经元的相位反应作为强化信号的作用。
这些方法允许神经科学家在清醒的动物中以毫秒的时间尺度精确地控制所选的特定类型的神经元活动。
光遗传学方法将光敏蛋白质引入选定类型的神经元中，使这些神经元可以通过激光闪光被激活或静默。
第一个使用光遗传学方法研究多巴胺神经元的实验显示，使小鼠产生多巴胺神经元相位激活的光遗传刺激会使小鼠更喜欢房间里接受刺激的一侧
（在房间的另一侧它们没有收到或只收到低频率的刺激（Tsai et al，2009）。
在另一个例子中， Steinberg等人（2013）利用多巴胺神经元的光遗传对老鼠身上的多巴胺神经元活动进行人为激活，
这时本该发生收益刺激但实际没有，多巴胺神经元活动通常暂停。
人为激活后，响应持续并由于缺少强化信号（在消退试验中）而正常地衰减，
由于收益已经被正确预测，所以学习通常会被阻塞（阻塞示例见本书14.2.1节）。

.. figure:: images/single_neuron_producing_dopamine.png
    :align: right
    :width: 250px

    单个神经元的轴突生成多巴胺作为神经递质。这些轴突通过突触和脑中目标区域的大量神经元树突进行信息传递。

    引自： The Journal of Neurosctence, Matsuda, Furuta, Nakamura, Hioki. Fujiyama,
    Arai, and Kaneko, volume 29, 2009, page 451.

多巴胺强化作用的另外证据来自果蝇的光遗传学实验，尽管这些动物中多巴胺的作用与哺乳动物中的作用相反：
至少对多巴胺神经元活化的群体来说，多巴胺神经元活性的光学触发像对脚电击一样来强化“回避行为”（Claridge－Chang－等，2009）。
虽然这些光遗传学实验都没有显示多巴胺神经元相位活动特别像TD误差，
但是它们有力地证明了多巴胺神经元相位活动像 :math:`\delta` 在强化信号预测（经典条件反射）和
控制（工具性条件反射）中那样起着重要作用（或许对果蝇来说像 :math:`minus \delta` 的作用）。

多巴胺神经元特别适合于向大脑的许多区域广播强化信号。
这些神经元具有巨大的轴突，每一个都能在比普通轴突多100~1000倍的突触位点上释放多巴胺。
右图显示了单个多巴胺神经元的轴突，其细胞体位于老鼠大脑的SNpc中。
每个spc或VTA多巴胺神经元的轴突在靶向大脑区域中的神经元树突上产生大约500 000个突触。

如果多巴胺神经元像强化学习 :math:`\delta` 那样广播强化信号，那么由于这是一个标量信号，即单个数字，
所以在SNpc和VTA中的所有多巴胺神经元会被预期以相同的方式激活，并以近似同步的方式发送相同的信号到所有轴突的目标位点。
尽管人们普遍认为多巴胺神经元确实能够像这样一起行动，但最新证据指出，
多巴胺神经元的不同亚群对输入的响应取决于它们向其发送信号的目标位点的结构，以及信号对目标位点结构的不同作用方式。
多巴胺具有传导RPE以外的功能。而且即使是传导RPE信号的多巴胺神经元，多巴胺也会将不同的RPE发送到不同的结构去，
这个发送过程是根据这些结构在产生强化行为中所起的作用来进行的。
这超出了我们讨论的范围，但无论如何，矢量值RPE信号从强化学习的角度看是有意义的，
尤其是当决策可以被分解成单独的子决策时，或者更一般地说，处理结构化的功劳分配问题时就更是如此。
所谓 *结构化功劳分配问题* 是指：如何为众多影响决策的结构成分分配成功的功劳收益（或失败的惩罚）？
我们会在15.10节中详细讨论这一点。

大多数多巴胺神经元的轴突与额叶皮层和基底神经节中的神经元发生突触接触，涉及自主运动、决策、学习和认知功能的大脑区域。
由于大多数关于多巴胺强化学习的想法都集中在基底神经节，而多巴胺神经元的连接在那里特密集，所以我们主要关注基底神经节。
基底神经节是很多神经元组（又称“神经核”）的集合，位置在前脑的基底。基底节的主要输入结构称为纹状体。
基本上所有的大脑皮层以及其他结构，都为纹状体提供输入。皮层神经元的活动传导关于感官输入、内部状态和运动活动的大量信息。
皮层神经元的轴突在纹状体的主要输入／输出神经元的树突上产生突触接触，称为中棘神经元。
纹状体的输出通过其他基底神经核和丘脑回到皮质的前部区域和运动区域，使得纹状体可能影响运动、抽象决策过程和收益处理。
纹状体的两个主要分叉对于强化学习来说十分重要：背侧纹状体，主要影响动作选择；和腹侧纹状体，在收益处理的不同方面起关键作用，包括为各类知觉分配有效价值。

中棘神经元的树突上覆盖着“棘”，该皮质神经元的尖端轴突有突触之间信息传递的功能。
这些棘也会参与突触之间的信息传递──在这种情况下连接的是脊柱茎，其是多巴胺神经元的轴突（图15.1）。
这样就将皮层神经元的突触前活动、中棘神经元的突触后活动和多巴胺神经元的输入汇集在一起。
实际上这些发生在脊柱茎上的过程很复杂，还没有被完全弄清楚。
图15.1通过显示两种类型的多巴胺受体──谷氨酸受体（谷氨酸受体的神经递质），以及各种信号相互作用的方式说明了这种活动的复杂性。
但有证据表明，神经科学家称之为皮质纹状体突触的从皮层到纹状体突触相关性的变化，取决于恰当时机的多巴胺信号。

.. figure:: images/figure-15.1.png

    **图15.1** 纹状神经元的脊柱茎的输入来自于皮层神经元和多巴胺神经元。
    皮层神经元轴突通过纹状体突触影响纹状神经元，神经递质谷氨酸在棘端覆盖纹状神经元树突。
    一个VTA或SNpc多巴胺神经元的轴突在脊柱茎旁边（图的右下方）。
    轴突上的“多巴胺膨体”在脊柱茎或附近释放多巴胺，在将皮层突触前输入、纹状体神经元突触后活动和多巴胺结合起来的组织方式中，
    这使得可能有几种类型的学习规则共同支配皮质纹状突触的可塑性。
    多巴胺神经元的每个轴突与大约500 000个脊柱茎的突触发生信息传递。
    其他神经递质传递途径和多种受体类型不在我们讨论范围，如D1和D2多胺受，多巴胺可以在脊柱和其他突触后位点产生不同的效应。
    引自： Journal of Neurophysiology，w. Schultz vol.80，1998，page10.


15.5 收益预测误差假说的实验支持
---------------------------------

多巴胺神经元以激烈、新颖或意想不到的视觉、听觉刺激来触发眼部和身体的运动，但它们的活动很少与运动本身有关。
这非常令人惊讶，因为多巴胺神经元的功能衰退是帕金森病的一个原因，其症状包括运动障碍，尤其是自发运动中的缺陷。
Romo和 Schultz（1990）以及Schultz和Romo（1990）通过记录猴子移动手臂时多巴胺神经元和肌肉的活动开始向收益预测误差假说迈出第一步。

他们训练了两只猴子，当猴子看见并听到门打开的时候，会把手从静止的地方移动到一个装有苹果、饼干或葡萄干的箱子里。
然后猴子可以抓住食物并吃到嘴里。当猴子学会这么做之后，它又接受另外两项任务的训练。
第一项任务的目的是看当运动是自发时多巴胺神经元的作用。箱子是敞开的，但上面被覆盖着，猴子不能看到箱子里面的东西，但可以从下面伸手进去。
预先没有设置触发刺激当猴子够到并吃完食物后，实验者通常（虽然并非总是）在猴子没看见的时候悄悄将箱中的食物粘到一根坚硬的电线上。
在这里，Romo和 Schultz观察到的多巴胺神经元活动与猴子的运动无关，但是当猴子首先接触到食物时，这些神经元中的大部分会产生相位反应。
当猴子碰到电线或碰到没有食物的箱子时这些神经元没有响应。这是表明神经元只对食物，而非任务中的其他方面有反应的很好的证据。

Romo和 Schultz第二个任务的目的是看看当运动被刺激触发时会发生什么。
这个任务使用了另外一个有可移动盖子的箱子。箱子打开的画面和声音会触发朝向箱子的移动。
在这种情况下，Romo和 Schultz发现，经过一段时间的训练后，多巴胺神经元不再响应食物的触摸，
而是响应食物箱开盖的画面和声音这些神经元的相位反应已经从收益本身转变为预测收益可用性的刺激。
在后续研究中，Romo和 Schultz发现，他们所监测的大多数多巴胺神经元对行为任务背景之外的箱子打开的视觉和声音没有反应。
这些观察结果表明，多巴胺神经元既不响应于运动的开始，也不响应于刺激的感觉特性，而是表示收益的期望。

Schultz的小组进行了许多涉及SNpc和VT多巴胺神经元的其他研究。
一系列特定的实验表明，多巴胺神经元的相位反应对应于TD误差，而不是像 Rescorla－Wagner模型（式（14.3）那样的简单误差。
在第一个实验中（Ljungberg、 Apicella Schultz，1992），训练猴子们在打开光照（作为“触发线索”）之后按压杠杆来获得一滴苹果汁。
正如Romo和 Schultz早些时候所观察到的，许多多巴胺神经元最初都对收益果汁滴下来有所回应（图15.2，上图）。
但是许多神经元在训练继续下去后失去了收益反应，而是转而对预测收益的光照有所反应（图15.2，中图）。
在持续的训练中，随着响应触发线索的多巴胺神经元变少，按压杠杆变得更快。

.. figure:: images/figure-15.2.png

    **图15.2** 多胺神经元的反应从最初的反应到初级收益再到早期的预测刺激的转变。
    图中展示的是在细微时间间隔内被监测的多巴胺神经元产生的动作电位（这些数据是23~4个神经元产生的）。
    顶部图：多巴胺神经元被无规律产生的苹果汁激活。中间图：随着学习，多巴胺神经元对收益预测触发线索产生反应，对收益传递失去反应。
    底部图：通过在触发脉冲之前增加1s的指示线索，多巴胺神经元将它们的响应从触发线索转移到较早的指示线索。
    引自： Schultz et al.（1995）， MIT Press.

在这项研究之后，同样的猴子接受了新的任务训练（Schultz、 Apicella和 Ljungberg，1993）。
这次猴子面临两个杠杆，每个杠杆上面都有一盏灯。点亮其中一个灯是一个“指示线索”，指示两个手柄中的哪一个会产生一滴苹果汁。
在这个任务中，指示线索先于触发提示产生，提前产生的间隔固定为1秒。
猴子要学着在看到触发线索之前保持不动，多巴胺神经元活动增加，
但是现在监测到多巴胺神经元的反应几乎全部发生在较早的指示线索上，而不是触发线索（图15.2，下图）。
在这个任务被充分学习时，再次响应指示线索的多巴胺神经元数量也大大减少了。
在学习这些任务的过程中，多巴胺神经元活动从最初的响应收益转变为响应较早的预测性刺激，首先响应触发刺激，然后响应更早的指示线索。
随着响应时间的提前，它在后面的刺激中消失。这种对后来的预测因子失去反应，而转移到对早期收益预测有所反应，是时序差分学习的一个标志（见图14.2）。

刚刚描述的任务也揭示了时序差分学习与多巴胺神经元活动共同具有的另一个属性。
猴子有时会按下错误的按键，即指示按键以外的按键因此没有收到任何收益。
在这些试验中，许多多巴胺神经元在收益正常给出后不久就显示其基线放电速率急剧下降，
这种情况发生时没有任何外部线索来标记通常的收益传送时间（图15.3）。
不知何故猴子在内部也能追踪收益传送的时间（响应时间是最简单的时序差分学习版本需要修改的一个地方，
以解释多巴胺神经元反应时间的一些细节，我们在下一节会考虑这个问题）。

上述研究的观察结果使 Schultz和他的小组得出结论：多巴胺神经元对不可预测的收益，最早的收益预测因子做出反应，
如果没有发现收益或者收益的预测因子，那么多巴胺神经元活性会在期望时间内降低到基线以下。
熟悉强化学习的研究人员很快就认识到，这些结果与时序差分算法中时序差分强化信号的表现非常相似。
下一节通过一个具体的例子来详细探讨这种相似性。

.. figure:: images/figure-15.3.png

    **图15.3** 多巴胺神经元的反应在预期收益衰退发生后不久就低于基线。
    顶部图：多巴胺神经元被无规律产生的苹果汁激活。中间图；多巴胺神经元对预测收益的条件刺激（CS）做出反应，并不对收益本身做出反应。
    底部图：当预测收益的条件刺激停止产生时，多巴胺神经元的活动会在期望的收益产生后的短时间内低于基线值。
    这些图的上部分显示的是所监测的多巴胺神经元在所指示的细微时间间隔内产生的动作电位的平均数目。
    这些图的下部分的光栅图显示了监测的单个多巴胺神经元的活动模式，每个点代表动作电位。
    引自： Schultz, Dayan, and Montague, A Neural Substrate of Prediction and Reward,
    Science, vol. 275, issue 5306, pages 1593-1598, March 14, 1997.
    经AAAS许可转载。


15.6 TD误差／多巴胺对应
----------------------------

这一节解释TD误差 :math:`\delta` 与实验中观察到的多巴胺神经元的相位反应之间的联系。
我们观察在学习的过程中如何变化，如上文中提到的任务一样，一只猴子首先看到指令提示，
然后在一个固定的时间之后必须正确地响应一个触发提示以获得收益。
我们采用种这个任务的简化理想版本，但是我们会更深入地研究细节，
因为我们想要强调TD误差与多巴胺神经元活动对应关系的理论基础。

第一个最基本的简化假设是智能体已经学习了获得收益的动作。接下来它的任务就是根据它经历的状态序列学习对于未来收益的准确预测。
这就是一个预测任务了，或者从更技术化的角度描述，是一个策略评估任务：针对一个固定的策略学习价值函数（4.1节和6.1节）。
要学习的价值函数对每一个状态分配一个值，这个值预测了如果智能体根据给定的策略选择动作则接下来状态的回报值，
这个回报值是所有未来收益的（可能是带折扣的）总和。
这对于猴子的情境来说是不实际的因为猴子很可能在学习正确行动的同时学习到了这些预测
（就像强化学习算法同时学习策略和价值函数，例如“行动器-评判器”算法），但是这个情境相比同时学习策略和价值函数更易于描述。

现在试想智能体的经验可以被分为多个试验，在每个试验中相同的状态序列重复出现，但在每个时刻的状态都不相同。
进一步设想被预测的收益仅限于一次试验，这使我们的每次试验类似于强化学习的一幕，正如我们之前所定义的。
在现实中，被预测的回报值不仅限于单个试验，且两个试验之间的时间间隔是决定动物学习到什么的重要影响因素。
这对于时序差分学习来说同样是真实的，但是在这里我们假设回报值不会随着多个试验逐渐积累。
在这种情况下，如 Schultz和他的同事们做的，一次实验中的一个试验等价于强化学习的一幕
（尽管在这个讨论中，我们用术语“试验”而不是“幕”来更好地与实验相联系）。

通常，我们同样需要对状态怎样被表示为学习算法的输入做出假设，这是一个影响TD误差与多巴胺神经元的活动联系有多紧密的假设。
我们稍后讨论这个问题，但是我们现在假设与 Montague相同的CSC表示，在实验中的每一个时刻，访问过的每一个状态都有一个单独的内部刺激。
这使得整个过程被简化到本书第I部分讨论的表格型的情况。
最终，我们假设智能体使用TD(0)来学习一个价值函数 :math:`V`，将其存储在一个所有状态初始值为零的查询表中。
我们同样假设这是一个确定的任务且折扣因子 :math:`\gamma` 非常接近于1，以至于我们可以忽略它。

图15.4展示了在这个策略评估任务中几个学习阶段中的 :math:`R`、:math:`V` 和 :math:`\delta` 的时间过程。
时间轴表示在一个试验中一系列状态被访问的时间区间（为了表达清楚，我们没有展示单独状态）。
除了在智能体到达收益状态外收益信号在整个试验中始终为零，如图中时间线右末端所示，收益信号成为一个正数，如 :math:`R^{\star}`。
时序差分学习的目标是预测在试验中访问过的每一个状态的回报值，
在没有折扣的情况下并且假设预测值被限制为针对单独试验，对于每个状态就是 :math:`R^{\star}`。

.. figure:: images/figure-15.4.png

    **图15.4** 时序差分学习中的TD误差 :math:`\delta` 的表现与多巴胺神经元相位活动特征完全一致。
    （这里的TD误差 :math:`\delta` 指的是 :math:`t` 时刻的误差：:math:`\delta_{t-1}`）。
    一个状态序列，通常情况下表示预测线索到收益之间的间隔，后面是非零收益R学习早期：初始化价值函数V和δ，一开始初始化为 :math:`R^{\star}`。
    学习完成：价值函数精确地预测未来收益，在早期的预测状态，:math:`\delta` 是正值，在非零收益时 :math:`\delta=0`。
    省略 :math:`R`：当省略预测收益时，:math:`\delta` 是负值。文中有这一现象的完整解释。

在得到真实收益的每个状态之前是一系列的收益预测状态，*最早收益预测状态* 被展示在时间线的最左端。
这个状态就像是接近试验开始时的状态，例如在上文中描述的 Schultz的猴子实验中的指令线索状态。
这是在试验中可以用来可靠预测试验收益的首个状态（当然，在现实中，在先前试验中访问过的状态可能是更早的收益预测状态，但是我们限制预测针地单独的试验，它们不能作为这个试验的收益的预测。
在下面我们给出一个更加令人满意的，尽管更抽象的，对于最早收益预测状态的描述）。
一个试验中的 *最近收益预测状态* 是指试验中收益状态的前一个状态。这个状态被表示为图15.4中时间线上最右端的状态。
注意一个试验的收益状态不能预测该试验的回报值：这个状态的值将被用来预测接下来所有试验的累积回报值，在当前分幕式的框架里我们假设这个回报值是零。

图15.4展示了 :math:`V` 和 :math:`\delta` 的首次试验的时间过程，在图中被标记为“学习早期”。
因为除了到达收益状态时的收益以外，试验中的所有信号都是零，且所有 :math:`V` 值都是零，
TD误差在它在收益状态变为 :math:`R^{\star}` 前都是零。
这个结果是由于 :math:`\delta_{t-1}=R_{t}+V_{t}-V_{t-1}=R_{t}+0-0=R_{t}`，
这个值在获得收益变为 :math:`R^{\star}` 前都是零。
在这里 :math:`V_t` 和 :math:`V_{t－1}`是在试验中时刻 :math:`t` 和 :math:`t－1` 访问状态的预测价值。
在这个学习阶段中的TD误差与多巴胺神经元对一个不可预知的收益的响应类似，例如在训练起始时的一滴苹果汁。

在首个试验和所有接下来的试验中，TD(0)更新发生在第6章中描述的每次状态转移中。
这样会随着收益状态的价值更新的反向传递，不断地增加收益预测状态的价值，直到收到正确的回报预测。
在这种情况下（假设没有折扣），正确的预测值对于所有收益预测状态都等于 :math:`R^{\star}`。
这可以在图15.4看出，在V的标有“学习完成”的图中，从最早到最晚的收益预测状态的价值都等于 :math:`R^{\star}`。
在最早收益预测状态前的状态的价值都很小（在图15.4中显示为0），因为它们不是收益的可靠预测者。

当学习完成时，也即当V达到正确的值时，因为预测现在是准确的，所以从任意收益预测状态出发的转移所关联的TD误差都是零，
这是因为对一个从收益预测状态到另一个收益预测状态的转移来说，
我们有 :math:`\delta_{t-1}=R_{t}+V_{t}-V_{t-1}=0+R^{\star}-R^{\star}=0`。
且对于最新的收益预测状态到收益状态来说，
我们有 :math:`\delta_{t-1}=R_{t}+V_{t}-V_{t-1}=R^{\star}+0-R^{\star}=0`。
在另一方面，从任意状态到最早收益预测状态转移的TD误差都是正的，这是由这个状态的低值与接下来收益预测状态的高值的不匹配造成的。
实际上，如果在最早收益预测状态前的状态价值为零，则在转移到最早收益预测状态后，
我们有 :math:`\delta_{t-1}=R_{t}+V_{t}-V_{t-1}=0+R^{\star}-0=R^{\star}`。
图15.4中的的“学习完成”图在最早收益预测状态为正值，在其他地方为零。

转移到最早收益预测状态时的正的TD误差类似于多巴胺对最早刺激的持续性反应，用以预测收益。
同样道理，当学习完成时，从最新的收益预测状态到收益状态的转移产生一个值为零的TD误差，因为最新收益预测状态的值是正确的，抵销了收益。
这与相比一个不可预测的收益，对一个完全可预测的收益，更少的多巴胺神经元产生相位响应的观察是相符的。

在学习后，如果收益突然被取消了，那么TD误差在收益的通常时间都是负的，
因为最新收益预测状态的值太大了：:math:`\delta_{t-1}=R_{t}+V_{t}-V_{t-1}=0+0-R^{\star}=-R^{\star}`，
正如图15.4中所示的标有“省略 :math:`R`”的 :math:`\delta` 图所示。
这就像在 Schultz et al.（1993）实验和图15.3中的多巴胺神经元行为，其在一个预测的收益被取消时会降低到基线以下。

需要更多地注意 *最早收益预测状态* 的概念。
在上文所提到的情境中，由于整个实验经历是被分为多次试验的，且我们假设预测被限制于单次试验，则最早收益预测状态总是试验中的第一个状态。
明显这不符合真实情况。一种考虑最早收益预测状态的更一般的方式是，认为它是一个不可预知的收益预估器，且可能有非常多这样的状态。
在动物的生活中，很多不同的状态都在最早收益预测状态之前。
然而，由于这些状态通常跟随着不能预测收益的其他状态，因此它们的收益的预测力，也就是说，它们的值，很低。
一个TD算法，如果在动物的一生中始终运行，也会更新这些状态的价值，但是这些更新并不会一直累积，
因为根据假设，这些状态中没有一个能保证出现在最早收益预测状态之前。
如果它们中的任意一个能够保证，它们也会是收益预测状态。
这也许解释了为什么经过过度训练，在试验中多巴胺的反应甚至降低到了最早的收益预测刺激水平。
经过过度训练，可以预料，就算是以前不能预测的状态都会被某些与更早的状态联系起来的刺激预测出来：
在实验任务的内部和外部，动物与环境的相互作用将变成平常的、完全可预测的事情。
但是，当我们通过引入新的任务来打破这个常规时我们会观察到TD误差重新出现了，正如在多巴胺神经元活动中观察到的那样。

上面描述的例子解释了为什么当动物学习与我们例子中的理想化的任务类似的任务时，TD误差与多巴胺神经元的相位活动有着共同的关键特征。
但是并非多巴胺神经元的相位活动的所有性质都能与 :math:`\delta` 的性质完美对应起来。
最令人不安的一个差异是，当收益比预期提前发生时会发生什么。
我们观察到一个预期收益的省略会在收益预期的时间产生一个负的预测误差，这与多巴胺神经元降至基线以下相对应。
如果收益在预期之后到达它就是非预期收益并产生一个正的预测误差。这在TD误差和多巴胺神经元反应中同时发生。
但是如果收益提前于预期发生，则多巴胺神经元与TD误差的反应不同──至少在 Montague et al.（1996）使用的CSC表示与我们的例子中不同。
多巴胺神经元会对提前的收益进行反应，反应与正的TD误差一致，因为收益没有被预测会在那时发生。
然而，在后面预期收益出现却没有出现的时刻，TD误差将为负，
但多巴胺神经元的反应却并没有像负的TD误差的那样降到基线以下（Hollerman和 Schultz，198）。
在动物的大脑中发生了相比于简单的用CSC表示的TD学习更加复杂的事情。

一些TD误差与多巴胺神经元行为的不匹配可以通过选择对时序差分算法合适的参数并利用除CSC表示外的其他刺激表示来解决。
例如，为了解决刚才提到的提前收益不匹配的问题，Suri和 Schultz（199）提出了一种CSC的表示，
在这种表示中由较早刺激产生的内部信号序列被出现的收益取消。
另一个由Daw、 Courville Touretzky（2006）提出的解决方法
是大脑的TD系统使用在感觉皮层进行的统计建模所产生的表示，而不是基于原始感官输入的简单表示。
Ludvig、 Sutton和 Kehoe2008）发现采用微刺激表示的TD学习比CSC表示更能在收益早期和其他情形下模拟多巴胺神经元的行为（见图14.1）。
Pan、 Schmidt、 Wickens和 Hyland（205）发现即使使用CSC表示，
延迟的资格迹可以改善TD误差与多巴胺神经元活动的某些方面的匹配情况。
一般来说，TD误差的许多行为细节取决于资格迹、折扣和刺激表示之间微妙的相互作用。
这些发现在不否定多巴胺神经元的相位行为被TD误差信号很好地表征的核心结论下细化了收益预测误差在另一方面，
在TD理论和实验数据之间有一些不能通过选择参数和刺激表示轻易假说。

在另一方面，在TD理论和实验数据之间有一些不能通过选择参数和刺激表示轻易解决的差异
（我们将在章末参考文献和历史评注的部分介绍某些差异），随着神经科学家进行更多细化的实验，更多的差异会被发现。
但是收益预测误差假说作为提升我们对于大脑收益系统理解的催化剂已经表现得非常有效。
人们设计了复杂的实验来证明或否定通过假设获得的预测，实验的结果也反过来优化并细化了TD误差／多巴胺假设。

一个明显的发展方向是，与多巴胺系统的性质如此契合的强化学习算法和理论完全是从一个计算的视角开发的，
没有考虑到任何多巴胺神经元的相关信息──注意，TD学习和它与最优化控制及动态规划的联系
是在任何揭示类似TD的多巴胺神经元行为本质的实验进行前很多年提出的。
这些意外的对应关系，尽管还并不完美，却也说明了TD误差和多巴胺的相似之处抓住了大脑收益过程的某些关键环节。

除了解释了多巴胺神经元相位行为的很多特征外，收益预测误差假说将神经科学与强化学习的其他方面联系起来，
特别地，与采用TD误差作为强化信号的学习算法联系起来。
神经科学仍然距离完全理解神经回路、分子机制和多巴胺神经元的相位活动的功能十分遥远，
但是支持收益预测误差假说的证据和支持多巴胺相位反应是用于学习强化信号的证据，
暗示了大脑可能实施类似的“行动器-评判器”算法，在其中TD误差起着至关重要的作用。
其他的强化学习算法也是可行的候选，但是“行动器-评判器”算法特别符合哺乳动物的大脑解剖学和生理学，我们在下面两节中进行阐述。

