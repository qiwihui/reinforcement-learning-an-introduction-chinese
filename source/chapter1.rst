第1章 简介
===========

当我们思考学习的本质时，我们首先想到的是通过与环境互动来学习。
当一个婴儿玩耍，挥动手臂或环顾四周时，他没有明确的老师，但他确实通过直接的感觉与环境联系。
他可以通过这种联系获得大量关于因果关系、行动的结果以及如何实现目标的信息。
在我们的生活中，这种互动无疑是我们环境和自身知识的主要来源。
无论我们是学习驾驶汽车还是进行交谈，我们都敏锐地意识到我们的环境如何响应我们的行为，并且我们试图通过我们的行为来影响所发生的事情。
从互动中学习是几乎所有学习和智能理论的基本思想。

在本书中，我们探索了一种从交互中学习的 *计算* 方法。
我们主要探索理想化的学习情境并评估各种学习方法的有效性，而不是直接理解人或动物的学习方式 [#学习方式]_ 。也就是说，我们采用人工智能研究员或工程师的观点。
我们探索在解决科学或经济利益的学习问题方面有效的机器设计，通过数学分析或计算实验评估设计。
我们探索的方法称为 *强化学习*，更侧重于从交互中进行目标导向的学习，而不是其他机器学习方法。

.. [#学习方式] 第14章和第15章总结了心理学和神经科学的关系。

1.1 强化学习
------------

强化学习是一种学习如何将状态映射到动作，以获得最大奖励的学习机制。
学习者不会被告知要采取哪些行动，而是必须通过尝试来发现哪些行动会产生最大的回报。
在最有趣和最具挑战性的案例中，行动不仅可以影响直接奖励，还可以影响下一个状态，并通过下一个状态，影响到随后而来的奖励。
这两个特征 - 试错法和延迟奖励 - 是强化学习的两个最重要的可区分特征。

强化学习，就像许多名称以“ing”结尾的主题一样，例如机器学习和登山，同时也是一个问题，一类能够很好地解决问题的解决方法，以及研究这个问题及其解决方法的领域。
为所有三件事情使用单一名称是方便的，但同时必须保持三者（问题，方法，领域）在概念上的分离。
特别的，在强化学习中，区分问题和解决问题的方法是非常重要的；没有做出这种区分是许多混乱的根源。

我们使用动态规划理论的思想来规范化强化学习的问题，具体地说，是不完全已知的马尔可夫决策过程的最优控制。
这种规范化的详细描述将在第3章，但是最基本的思想是：采样实际问题最重要的方面，训练一个智能体多次与环境交互去达到一个目标。
智能体必须能够在某种程度上感知其环境状态，并且能够采取行动影响环境的状态。
智能体还必须具有与环境状态相关的一个或多个目标。
马尔科夫决策过程基本概念无差别的包含感知，行动和目标三个方面。
我们认为任何非常适合解决此类问题的方法都是一种强化学习方法。

强化学习不同于 *监督学习*。
监督学习是目前机器学习领域中研究最多的一种学习方式，它从知识渊博的教练所提供的有标记的训练集中学习。
每一个样例都是一种情况的描述，都带有标签，标签描述的是系统在该情况下的应该采取的正确动作，每一个样例用来区别这种情况应该属于哪一类。
这种学习的目的是让系统推断或概括它应有的反馈机制，使它可以对未知样本作出正确回应。
这是一种重要的学习方式，但单凭它并不足以从交互中学习。
在交互问题中，找到期待的既正确又典型的例子通常都是不切实际的。
在一个未知的领域，若要使收益最大化，智能体必须能够从自己的经验中学习。

强化学习也与机器学习研究人员所谓的 *无监督学习* 不同，后者通常是寻找隐藏在未标记数据集合中的结构。
监督学习和无监督学习这两个术语似乎对机器学习范式进行了详尽的分类，但事实却并非如此。
尽管人们可能会试图将强化学习视为一种无监督学习，因为它不依赖于正确行为的样例，强化学习试图最大化奖励信号而不是试图找到隐藏的结构。
在智能体的经验数据中揭示结构确实对强化学习特别有用，但是它本身并没有解决最大化奖励信号的强化学习问题。
因此，我们认为强化学习是第三种机器学习范式，除此之外还有监督学习和无监督学习，也许还有其他范式。

在强化学习中出现的其他类型的学习中未出现的挑战之一，是如何权衡探索与开发之间的关系。
为了获得大量奖励，强化学习智能体必须倾向于过去已经尝试过并且能够有效获益的行动。
但是要发现这样的行为，它必须尝试以前没有选择的行为。
智能体必须充分 *利用* 它既有经验以获得收益，但它也必须 *探索*，以便在未来做出更好的行动选择。
困境在于，任何探索和开发都难以避免失败。
智能体必须尝试各种行动，逐步地选择那些看起来最好的行动。
在随机任务中，每一个动作必须经过多次尝试才能得到可靠的预期收益。
几十年来，数学家一直在深入研究探索开发困境，但仍未得到解决。
就目前而言，在监督和无监督的学习中，至少在这些范式的最纯粹的形式中，完全平衡探索和开发的项目尚未出现。

强化学习的另一个关键特征是它明确地考虑了目标导向的智能体与不确定环境相互作用的 *整个* 问题。
这与许多考虑子问题但没有解决它们如何融入更大的图景的方法形成对比。
例如，我们已经提到很多机器学习研究都关注监督学习而没有明确说明这种能力最终如何有用。
其他研究人员已经制定了具有总体目标的规划理论，但没有考虑规划在实时决策中的作用，也没有考虑规划所需的预测模型来自何处的问题。
尽管这些方法已经产生了许多有用的结果，但它们一个重要的限制在于过于关注孤立子问题。

强化学习采取相反的策略，它具有一个完整的、交互式的、寻求目标的智能体。
所有强化学习智能体都有明确的目标，可以感知环境的各个方面，并可以选择影响其环境的行为。
此外，通常从一开始就假设智能体必须必须操作，尽管它面临的环境有很大的不确定性。
当强化学习涉及规划时，它必须解决规划和实时行动选择之间的相互作用，以及如何获取和改进环境模型的问题。
当强化学习涉及监督学习时，它要确定决定哪些能力是关键的，哪些是不重要的原因
为了学习研究以取得进步，必须隔离和研究重要的子问题，但它们应该是在完整的、交互式的、寻求目标的智能体中有明确功能的子问题，即使不能体现所有完整的细节。

一个完整的、交互式的、寻求目标的智能体，并不总是意味着像是一个完整的有机体或机器人
这里有许多明显的例子，但是一个完整的、交互式的、寻求目标的智能体也可以是更大行为系统的一个组成部分。
在这种情况下，智能体直接与较大系统的其余部分交互，并间接与较大系统的环境交互。
一个简单的例子是一个智能体，它监控机器人电池的充电水平并向机器人的控制架构发送命令。
这个智能体的环境是机器人的其余部分以及机器人的环境。
人们的眼光应超越最明显的智能体及其环境的例子，才能理解强化学习框架的普遍性。

现代强化学习最激动人心的一个方面是与其他工程和科学学科的实质性和富有成效的互动。
强化学习是人工智能和机器学习领域长达数十年的一个趋势，它与统计学、最优化和其他数学学科更紧密地结合在一起。
例如，某些强化学习方法学习参数的能力解决了运筹学与控制论中的经典“维数灾难”。
更有特色的是，强化学习也与心理学和神经科学有着紧密的联系，双方都有很大的好处。
在所有形式的机器学习中，强化学习最接近人类和其他动物所做的学习，而强化学习的许多核心算法最初都受到生物学习系统的启发。
通过动物学习的心理模型返回更符合经验数据的结果，以及通过一部分大脑奖励系统的有影响力的模型，强化学习也得到了反馈。
本书正文介绍了与工程和人工智能相关的强化学习的思想，并在第14章和第15章中总结了与心理学和神经科学的联系。

最后，强化学习也在某种程度上符合人工智能回归简单的一般性原则的一个大趋势。
自20世纪60年代后期以来，许多人工智能研究人员认为普遍性的原则是不存在的，而智能则归因于拥有大量特殊用途的技巧，过程和启发式方法。
有人说，如果我们能够将相关的事实充分地提供给一台机器，比如一百万或十亿，那么它就会变得聪明起来。
基于一般原则（如搜索或学习）的方法被定性为“弱方法”，而基于特定知识的方法被称为“强方法”。
这种观点在今天仍然很普遍，但并不占优势。
从我们的观点来看，这只是一个不成熟的过程：寻找一般原则的努力太少，以至于没有结论。
现代人工智能现在包括许多研究，寻找学习，搜索和决策的一般原则。
目前还不清楚钟摆会摆动多远，但强化学习研究肯定是摆向更简单和更少的人工智能一般原则的钟摆的一部分。

1.2 例子
--------

理解强化学习的一个好方法是思考其发展中的一些例子和可能的应用。

- 国际象棋大师落子。这个决定既通过计划 - 期待的回复和逆向回复，也出于对特定位置和移动及时直觉的判断。
- 自适应控制器实时调节炼油厂操作的参数。控制器在指定的边际成本的基础上优化产量/成本/质量交易，而不严格遵守工程师最初建议的设定点。
- 一头瞪羚在出生后几分钟挣扎着站起来。半小时后，它以每小时20英里的速度奔跑。
- 移动机器人决定是否应该进入新房间以寻找和收集更多垃圾来，或尝试回到充电站充电。
  它根据电池的当前充电水平，以及过去能够快速轻松地找到充电器的程度做出决定。
- 菲尔准备他的早餐。仔细检查，即使是这个看似平凡的活动，也会发现一个复杂的条件行为网和互锁的目标-子目标关系：
  走到橱柜，打开它，选择一个谷物盒，然后伸手去拿，抓住并取回盒子。
  需要其他复杂的，调整的，交互的行为序列来获得碗，勺子和牛奶盒。每个步骤都涉及一系列眼球运动，以获取信息并指导到达和运动。
  它们需要对于如何拿住物品或者在拿其他物品之前将它们中的一些运送到餐桌上做出快速判断。
  每个步骤都以目标为指导并为其他目标服务，例如抓勺子或走到冰箱，比如一旦谷物准备好就那勺子吃以最终获得营养。
  无论他是否意识到这一点，菲尔都在获取有关他身体状况的信息，这些信息决定了他的营养需求，饥饿程度和食物偏好。

这些示例共享的功能非常基本，很容易被忽略。
所有这些都涉及积极的决策智能体与其环境之间的 *互动*，智能体在存在不确定性的环境中寻求实现 *目标*。
智能体的行为能影响未来的环境状态（例如，下一个国际象棋位置，炼油厂的水库水位，机器人的下一个位置以及其电池的未来充电水平），从而影响智能体之后可以采取的行动和机会。
正确的选择需要考虑到行动的间接延迟后果，因此可能需要预见或规划。

同时，在所有这些例子中，行动的效果都无法完全预测; 因此，智能体必须经常监控其环境并做出适当的反应。
例如，菲尔必须观察他倒入谷物碗中的牛奶以防止溢出。
所有这些例子都涉及明确的目标，即智能体可以根据其直接感知的内容判断实现目标的进度。
国际象棋选手知道他是否获胜，炼油厂控制员知道生产了多少石油，瞪羚小牛知道它何时落下，移动机器人知道它的电池何时耗尽，菲尔知道他是否正在享用他的早餐。

在所有这些示例中，智能体可以使用其经验来改善其性能。
国际象棋选手改进了他用来评估位置的直觉，从而改善了他的发挥; 瞪羚小牛提高了它的活力; 菲尔学会精简他的早餐。
智能体在任务开始时所具有的的知识 - 无论是之前的相关任务经验还是通过设计或演变带来的 - 都会影响有用或易于学习的内容，
但与环境的交互对于调整行为以利用任务的特性更加至关重要。

1.3 强化学习的要素
------------------

在智能体和环境之外，强化学习系统一般有四个主要元素：*一个策略*，*奖励信号*，*价值函数*，和一个可选的环境 *模型*。

*策略* 定义了学习智能体在给定时间内的行为方式。
简单来说，策略是从感知的环境状态到在这些状态下要采取的行动的映射。
它对应于心理学中所谓的一组刺激-反应规则或关联。
在某些情况下，策略可以是简单的函数或查找表，而在其他情况下，它可能涉及广泛的计算，例如搜索过程。
策略是强化学习智能体的核心，因为它本身就足以确定行为。一般来说，策略对指定每个动作的概率而言可以是随机的。

*奖励信号* 定义了强化学习问题的目标。
在每个时间步骤，环境向强化学习智能体发送的单个数字称为奖励。
智能体的唯一目标是最大化其长期收到的总奖励。
因此，奖励信号定义了相对智能体而言的好事和坏事。
在生物系统中，我们可能会认为奖励类似于快乐或痛苦的经历。
它们是智能体所面临的问题的直接和明确特征。
奖励信号是改变政策的主要依据; 如果策略选择的操作之后是低奖励，则可以更改策略以在将来选择该情况下的某些其他操作。
通常，奖励信号可以是环境状态和所采取的动作的随机函数。

虽然奖励信号表明了直接意义上的好处，但 *价值函数* 指定了长期利益。
粗略地说，一个状态的价值是智能体从该状态开始在未来可以预期累积的收益总额。
虽然奖励决定了环境状态的直接，内在的价值，但价值表明了在考虑到可能遵循的状态和这些状态下可获得的奖励之后各状态的长期价值。
例如，一个状态可能总是会产生较低的即时奖励，但仍然具有较高的价值，因为其他状态经常会产生高回报。
或者正相反。
以人类作类比，奖励有点像快乐（如果是高的奖励）和痛苦（如果是低的奖励），而价值则对应于我们对环境处于特定状态的高兴或不满的更精确和有远见的判断。

奖励在某种意义上是主要的，而作为奖励预测的价值是次要的。
没有奖励就没有价值，估算价值的唯一目的就是获得更多回报。
然而，在制定和评估决策时，我们最关心的是价值。
行动选择基于价值判断。
我们寻求带来最高价值状态的行动，而不是最高回报状态的行动，因为从长远来看，这些行动会为我们带来最大的回报。
不幸的是，确定价值要比确定奖励要困难得多。
奖励基本上由环境直接给出，但必须根据智能体在其整个生命周期中所做的观察序列来估计和重新估计价值。
事实上，我们考虑的几乎所有强化学习算法中最重要的组成部分是一种有效估计价值的方法。
价值估计的核心作用可以说是过去六十年中有关强化学习的最重要的事情。

一些强化学习系统具有第四个也是最后一个要素，既环境 *模型*。
这是对环境的模拟，或者更一般地说，它对环境的行为做出推断。
例如，给定状态和动作，模型可以预测结果的下一状态和下一个奖励。
模型用于 *规划*，我们指的是在实行动前对未来进行预判。
使用模型和规划解决强化学习问题的方法被称为 *基于模型* 的方法，而不是更简单的 *无模型* 方法，
无模型方法几乎被看作是规划的 *反面*，它通过试错进行学习。
在第8章中，我们将探索强化学习系统，它们通过反复尝试来学习，学习环境模型，并使用模型进行规划。
现代强化学习已经从低级、尝试和错误学习跨越到高层次的、有计划的学习。

1.4 局限性和范围
----------------

强化学习在很大程度上依赖于状态的概念 - 作为策略和价值函数的输入，以及模型的输入和输出。
非正式地，我们可以将状态视为向智能体传达某种特定时间“环境如何”的信号。
我们在这里使用的状态的正式定义由第3章中提出的马尔可夫决策过程的框架给出。
然而，更一般地，我们鼓励读者遵循非正式意义并将状态视为智能体对其环境所能获得的任何信息。
实际上，我们假设状态信号是由某些预处理系统产生的，而预处理系统是智能体环境的一部分。
在本书中，我们没有讨论构造、改变或学习状态信号的问题（除了第17.3节中的简要说明）。
我们采用这种方法并不是因为我们认为状态不重要，而是为了完全关注决策问题。
换句话说，我们在本书中的关注不是设计状态信号，而是设计行为函数以应对各种状态。

我们在本书中所考虑的大部分强化学习方法都是围绕估计价值函数构建的，但它对于解决强化学习问题而言并不是必须的。
例如，诸如遗传算法，遗传规划，模拟退火和其他优化方法的解决方法已被用于研究强化学习问题，而不必求助于值函数。
这些方法应用多个静态策略，每个策略在较长时间内与单独的环境实例进行交互。
获得最多奖励的策略及其随机变化将延续到下一代政策，并重复该过程。
我们称这些进化方法是因为它们的操作类似于生物进化方式，这种方式产生具有熟练行为的生物，即使它们在个体生命期间内不学习。
如果策略空间足够小，或者容易被构造，或者如果有大量的时间可用于搜索，那么进化方法可能是有效的。
此外，进化方法在学习智能体不能感知环境的完整状态的问题上具有优势。

我们的重点是强化学习方法，这些方法在与环境交互时学习，而进化方法则不然。
在许多情况下，能够利用个体行为交互细节的方法比进化方法更有效。
进化方法忽略了对大量的强化学习问题的有用结构：他们没有利用策略是从状态到行动的映射这一事实；他们并没有注意到个体生命周期中所经历的状态和采取的行动。
在某些情况下，这些信息可能会产生误导（例如，当状态不确定的时候），但更经常的是它能使搜索更有效率。
虽然进化和学习有许多共同的特性并且自然地协同工作，但我们并不认为进化方法本身特别适合强化学习问题，因此，我们不在本书中讨论它们。
